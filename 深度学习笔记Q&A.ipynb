{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 机器学习模型评估\n",
    "也称为性能度量(performance measure)。设计好了一个模型，如何去评价其泛化能力呢？<br>\n",
    "对于给定样例集$D={(x_1,y_1),(x_2,y_2)..(x_m,y_m)}$,其中$y_i$是真实标记，$x_i$,\n",
    "### **回归任务**\n",
    "最常用的性能度量是MSE(mean squared error)\n",
    "\n",
    "$E(f;D)=\\frac {1}{m}\\sum _{i=1}^m(f(x_i)-y_i)^2$\n",
    "\n",
    ">在一些回归任务诸如人群密度估计(crowd counting)中，也采用MAE(Mean Absolute Error)作为评价指标。\n",
    "### 分类任务的性能度量\n",
    "#### 错误率(Error)与精度(accuracy)\n",
    "**精度(acc)定义:**<br>\n",
    "$acc(f:D)=\\frac{1}{m}\\sum _{i=0}^m\\Pi(f(x_i)=y_i)$\n",
    "\n",
    "**错误率(Error)定义**<br>\n",
    "1-acc\n",
    "\n",
    "### 常见的一些范数及应用场景，如L0，L1,L2,Frobenius范数\n",
    "\n",
    "**理解范数：**平方范数$L^2$常用来衡量向量的大小，可以简单地通过$X^TX$来计算。直观上来讲，向量`x`的范数衡量从原点到点`x`的距离。\n",
    "$$||x||_p=\\left(\\sum|x_i|^p\\right)^\\frac{1}{p}$$\n",
    "\n",
    "\n",
    "范数在机器学习的应用场景：**正则化项**\n",
    "\n",
    "\n",
    "**正则化问题的引出：**\n",
    "\n",
    "机器学习中的一个核心问题是：**设计不仅在训练数据上表现好，而且能在新的输入(test集)上泛化好的算法。**正则化就是显式地设计来减少**测试误差**(可能会增大**训练误差**为代价)。\n",
    "在机器学习中，经常会遇到过拟合的现象\n",
    "有些正则化策略向目标函数增加额外项来对参数值进行软约束，如果我们细心选择，这些额外的约束和惩罚可以改善模型在测试集上的表现。有时候，这些约束和乘法被设计为编码特定类型的先验知识；其他时候，这些约束和乘法被设计为偏好简单模型，以便提高泛化能力(模型过于复杂，存在在训练集上过拟合的风险，相对简单的模型可能存在更强的泛化能力)。\n",
    "\n",
    "许多正则化方法通过对目标函数$J(\\theta;X,y)$添加一个参数惩罚$\\alpha \\Omega(\\theta)$\n",
    "当我们的训练算法最小化正则化后的木变函数$\\hat J(\\theta)$时，他会降低原始目标$J$关于训练误差并同时减小在某些衡量标准下的参数规模。选择不同的参数$\\theta$的范数会产生偏好不同的解。\n",
    "\n",
    "#### $L^2$参数正则化\n",
    "最简单最常见的参数惩罚范数惩罚，即通常被称为**权重衰减(weight decay)**的$L^2$的参数惩罚。这个正则化策略通过向目标函数添加一个正则化项使权重更加接近原点。$L^2$也被称为岭回归\n",
    "$$w=w-lr*(\\alpha w + \\nabla J(w))=(1-lr*\\alpha )w-lr*\\nabla J(w)$$\n",
    "加入权重衰减后会引起学习规则的衰减，即在没不执行通常的梯度更新之前先收缩权重向量。<br>\n",
    "\n",
    "$L^2$（**权重衰减**）正则化对最佳w值的影响,\n",
    "只有在显著减小目标函数方向上的参数$w$会保留得相对较好，在无助于目标函数减小的方向(对应Hessian矩阵较小特征值)上改变参数不会显著地增加梯度。这种不重要方向对应分量会在训练过程中因正则化而衰减掉\n",
    "\n",
    "#### $L^1$正则化\n",
    "L^2权重衰减是权重衰减是最常见的形式，还有其他方法限制模型参数的规模，一个选择是使用$L^1$正则化。\n",
    "$$\\Omega(\\theta)=\\sum |w_i|$$\n",
    "$$\\hat J(w;X,y)=\\alpha ||w||_1+J(w;X,y)$$\n",
    "对应的梯度\n",
    "$$\\hat J(w)=\\alpha sign(w)+\\nabla_wJ(w)$$\n",
    "其中$sign(w)$只是简单地取w各个元素的正负号\n",
    "我们可以看到正则化对梯度的影响不再是线性地缩放到每个$w_i$\n",
    "相比较$L^2$正则化，**$L^1$会产生更稀疏的解。此处稀疏性指的是最优值中的一些参数w为0**。$L^2$正则化不会使参数变得稀疏，而$L^1$正则化有可能通过足够大的$\\alpha$实现稀疏。\n",
    "\n",
    "\n",
    "由$L^1$正则化的稀疏性质已经被广泛地用于特征选择(feature selection)机制。\n",
    "#### 为何L1和L2正则化可以防止过拟合\n",
    "* L1&L2正则化会使模型偏好于更小的权值\n",
    "* 更小的权值意味着**更低的模型复杂度**；添加L1&L2相当于为模型添加了某种先验，限制了参数分布，从而降低了模型复杂度\n",
    "* 模型复杂度降低，意味着模型对于噪声与异常点的抗干扰能力增强，从而提高模型的泛化能力\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### 参数初始化\n",
    "一般使用服从标准正态分布(高斯分布)mean=0, std=1，或者均匀分布的随机值作为**权重**的初始化参数；使用0作为偏置的初始化参数\n",
    "一些启发式方法会根据输入与输出的单元数来决定初始值的范围\n",
    "* 随机正交觉着呢\n",
    "\n",
    "### Dropout策略\n",
    "Dropout通过**参数共享**提供了一种廉价的Bagging集成近似——Dropout策略相当于继承了包括所有从基础网络出去部分单元后形成的子网络\n",
    "通常，隐藏层的采样概率为0.5,输入的采样概率为0.8,；超参数也可以采样，但其采样概率一般为1\n",
    "\n",
    "### BatchNormalization批正则化\n",
    "\n",
    "训练的本质是**学习数据的分布**,如果训练数据与测试数据的分布不同会降低模型的泛化能力。因此，应该在开始训练器时对所有的数据做归一化处理，\n",
    "而在神经网络中，每个隐藏层参数不同，会使下一层的输入发生变化，从而导致每一批数据的分布也发生变化；导致网络在每次迭代中都需要你和不同的数据分布，增大了网络的训练难度与过拟合的风险。\n",
    "BN方法会针对每一批数据，在网络的每一层激活之前做归一化处理，使输入的均值为0，标准差为1.目的是将数据限制在统一的分布下\n",
    "\n",
    "#### 基本原理\n",
    "具体来说(针对DNN，全连接层构成的网络)，针对每一层第k个神经元，计算**批数据**在第`k`个神经元的均值与方差，然后将归一化后的值作为该神经元的激活值\n",
    "$$\\hat x_k=\\frac {x_k-E[x_k]}{\\sqrt{Var[x_k]}}$$\n",
    ">计算$E[x_k]$和$Var[x_k]$使用batch维度做平均，可以看成在batch上的白化操作\n",
    "\n",
    "BN对**层间数据分布进行额外约束**，但是这样会降低模型的拟合能力，破坏了之前学到的**特征分布**,为了**恢复数据的原始分布**，BN引入了一个**重构变化**，也就是可学习的$\\gamma$和$\\beta$\n",
    "\n",
    "训练的时候通过全局的batch计算出了期望和方差，但是在测试的时候，通常测试数据只有一个，怎么办呢？\n",
    "\n",
    "使用**全局统统计量**来替代批统计量\n",
    "训练每个batch时，都会得到一组`(均值，方差)`\n",
    "根据概率论学过的使用**样本无偏估计**，即在训练的时候就要计算这两个参数用于推断(BN层里面藏了不少东西^_^)\n",
    "$E[x]=E[\\mu_i]$<br>\n",
    "$Var[x]=\\frac{m}{m-1}E[\\sigma_i^2]$\n",
    "\n",
    "\n",
    "batch Normalizaiton作为近一年来DL的重要成果，已经被广泛证明其有效性和重要性。虽然有些细节处理还解释不清\n",
    "机器学习领域有一个很重要的假设：IID(独立同分布假设)，就是假设训练数据和测试数据是满足相同分布的，这是通过训练数据获得的模型能够在测试集获得好的效果的一个基本保证。Batch Normalization就是在深度神经网络训练过程中使得每一层神经网络的输入保持相同的分布\n",
    "\n",
    "##### 随着网络深度加深，训练起来困难，收敛越来越慢？\n",
    "这个是DL领域很接近本质的好问题。很多论文都是来解决这个问题的，例如RelU和ResNet\n",
    "对于深度学习这种包含很多隐藏层网络结构，在训练过程中，因为各层参数在不停地变化，所以每个隐藏层都会面临covariate shift(ICS)问题，也就是在训练过程中，隐藏层输入分布老是变来边去，提出了BatchNorm基本思想：能不能让每个隐藏层的节点的激活输入分布固定下来了？BN出现了..\n",
    "\n",
    "\n",
    "为什么随着网络深度加深，训练起来越来越困难，收敛越来越慢？从论文的\n",
    "mini-sgd对于One exmaple的两个优势：梯度更新方向更准确；并行计算速度更快；\n",
    "covariate shift：如果ML系统实例集合<X,Y>中的输入值X的分布老师变，这不符合IID假设。对于深度学习这种包含很多隐藏层的网络结构，在训练过程中，因为各层参数在不停变化，所以每个因曾都会面临covariate shift问题，也就是在训练过程中，隐藏层的输入分布老是变来边去\n",
    "\n",
    "### BN的推理(Inference)过程\n",
    "**思考：**BN在训练的时候可以根据Mini-Batch里的若干训练实例进行激活函数调整，但是在推理过程中，很明显输入只有一个实例，看不到Mini-Batch其他实例，无法求均值和方差那么这时候如何对输入做BN呢？\n",
    "既然没有从Mini-Batch数据中可以得到的统计量，那就想其他办法来获得均值方差。可以从所有的训练实例中获得统计量来代替Mini-Batch里面的m个训练实例\n",
    "\n",
    "### BN的优缺点再次总结\n",
    "**优点：**<br>\n",
    "1.加速网络学习收敛(消除了ICS，支持更大的学习率)，提高模型准确率\n",
    "2.防止了过拟合\n",
    "3.降低了对参数初始化的需求\n",
    "**缺点：**\n",
    "1. 十分依赖Batch size，一定要batch size足够大，效果才好；\n",
    "2. 不适合用于序列数据神经网络RNN，\n",
    "\n",
    "### 其他的归一化方式介绍\n",
    "layer Norm是在batch上面，对NHW做归一化，对小batchsize效果不好\n",
    "InstanceNorm实在图像像素上，对HW做归一化，用在风格迁移\n",
    "GroupNorm将channel分布，然后再做归一化\n",
    "SwitchableNorm是将BN,LN,IN结合，赋给权重，让网络自己学习归一化应该用什么方法\n",
    "\n",
    "\n",
    "\n",
    "### 简单介绍一下贝叶斯概率与频率派概率，以及在统计中对于真实参数的假设\n",
    "**答：**\n",
    "直接与时间发生的频率相联系，被称为频率派概率;而后者涉及确定性水平，被称为贝叶斯概率\n",
    "关于不确定性的常识推理，如果我们已经列出了若干条期望它具有的性质，那么满足这些性质的唯一方法就是将贝叶斯概率和频率派概率视为等同\n",
    "\n",
    "### 介绍一下sigmoid, relu, tanh, RBF及应用场景\n",
    "**答：**\n",
    "\n",
    "#### sigmoid \n",
    "$$g(z)=\\frac{1}{1+e^(-z)}$$\n",
    "特点：求导方便；值域(0,1)；头尾有饱和效应，对输入微小变化不敏感;在大部分定义域内饱和\n",
    "$$\\frac{d}{dz}g(z)=\\frac{1}{1+e^{-z}}(1-\\frac{1}{1+e^{-z}})=g(z)(1-g(z))$$\n",
    "\n",
    "#### tanh\n",
    "$$g(z)=tanh(z)=\\frac{e^z-e^{-z}}{e^z+e^{-z}}$$ \n",
    "\n",
    "#### ReLU？？缓解梯度爆炸？\n",
    "ReLU通常是比较好的激活函数\n",
    "**ReLU相比sigmoid的优势**\n",
    "1.缓解了sigmoid的梯度饱和区域，缓解了梯度消失\n",
    "2.计算起来更加方便，加速计算，在负半区域梯度为0，正半区域为1(0处导数为0，人为定义了导数保证了梯度的可学习性)\n",
    "#### softmax\n",
    "很自然地表示了具有k个可能值的离散型随机变量的概率分布\n",
    "\n",
    "#### softplus\n",
    "$$g(z)=log(1+exp(z))$$\n",
    "通过图像可以看到是ReLU的平滑版本，但从经验来看，并没有\n",
    "#### 为什么要使用非线性激活函数？\n",
    "使用**激活函数**的目的是为了向网络中加入**非线性因素**，从而加强网络的表示能力，解决**线性模型**无法解决的问题\n",
    "* 如果不适用非线性函数，整个网络都是一个线性组合函数\n",
    "#### 为什么加入非线性因素能够加强网络的表示能力？——神经网络的万能近似定力\n",
    "* 神经网络的万能近似定力认为给予网络足够数量的隐藏单元，他就能可以以任意的精度来映射**从一个有限空间到另一个有限空间的函数**\n",
    "\n",
    "## 损失函数\n",
    "交叉熵是非负的，在神经元达到很好的正确率的时候会接近0。这些其实就是我们想要的代价函数的特性。其实这些特性也是二次代价函数具备的。所以，交叉熵就是很好的选择了。交叉熵代价函数有一个比二次代价函数更好的特性就是它避免了学习速度下降的问题\n",
    "\n",
    "$$\\frac{\\partial C}{\\partial w_j}=-\\frac{1}{n}\\sum (s)$$\n",
    "\n",
    "#### 感受野(Receptive filed)\n",
    "在卷积神经网络CNN中，决定某一层输出结果中一个元素所对应的输入层的区域大小，被称作感受野\n",
    "\n",
    "### 选择超参数\n",
    "手动选择和自动选择。自动选择往往需要更高的计算成本\n",
    "学习率可能是最重要的超参数。如果你只有时间调整一个超参数，那就是调整学习率！！\n",
    "当学习率适合优化问题是，模型的有效容量最高，此时学习率是正确的，学习率关于训练误差具有U型曲线，，当学习率过大时，梯度下降可能会不经意地增加而非减少训练误差。当学习率太小时，训练不仅慢，还有可能永久听列在一个很高的训练误差上。\n",
    "\n",
    "#### 自动超参数优化算法\n",
    "1. 网格搜索\n",
    "通常当超参数量较小的时候，可以使用网格搜索法。即列出每个超参数的大致候选集合。利用这些集合进行逐项组合优化。在条件允许的情况下，重复进行网格搜索会相当优秀，当然每次重复需要根据上一步得到的最优参数组合，进行进一步的细粒度调整。网格搜索最大的问题在于计算时间会随超参数的数量指数级增长。\n",
    "\n",
    "2. 随机搜索\n",
    "不需要设定一个离散的超参数集合，而是对每个超参数定义一个分布函数来生成随机超参数。例如在batch size=[16,32,64]中随机搜索\n",
    "\n",
    "### softmax求导\n",
    "当i=j:<br>\n",
    "$S_i(1-S_i)$\n",
    "\n",
    "当i$\\ne$j:<br>\n",
    "\n",
    "\n",
    "大规模分布式实现\n",
    "模型并行\n",
    "多个机器共同运行同一个数据点，每个机器负责模型的一部分。\n",
    "\n",
    "数据并行\n",
    "\n",
    "## 随机梯度下降(SGD)\n",
    "在机器学习优化算法中，GD（gradient descent）是最常用的方法之一，简单来说就是在整个训练集中计算当前的梯度，选定一个步长进行更新。GD的优点是，基于整个数据集得到的梯度，梯度估计相对较准，更新过程更准确。但也有几个缺点，一个是当训练集较大时，GD的梯度计算较为耗时，二是现代深度学习网络的loss function往往是非凸的，基于凸优化理论的优化算法只能收敛到local minima，因此使用GD训练深度神经网络，最终收收敛点很容易落在初始点附近的一个local minima，不太容易达到较好的收敛性能。折中的方案就是mini-batch，一次采用batch size的sample来估计梯度，这样梯度估计相对于SGD更准，同时batch size能占满CPU/GPU的计算资源，又不像GD那样计算整个训练集。同时也由于mini batch能有适当的梯度噪声[8]，一定程度上缓解GD直接掉进了初始点附近的local minima导致收敛不好的缺点，所以mini-batch的方法也最为常用。\n",
    "\n",
    "**我们平时提到的SGD通常就是用mini-batch梯度下降**，mini-batch来确定当次\n",
    "通过计算少量样本的平均值可以快速得到一个对于实际梯度$\\nabla C_x$，来估算梯度，加速学习过程,当然mini-batch也可以是在总的数据集中也可以使随机的。\n",
    "\n",
    "小批量的大小通常由以下几个因素决定：\n",
    "* 更大的批量会计算更精确地梯度估计，但也不是越大越好，超过一定的范围会降低\n",
    "极小批量通常难以充分利用多核并行计算架构(如果计算量太小，即使多卡也没有什么太好的加速效果，因为单卡足以胜任工作)\n",
    "* 如果批量处理中的所有样本可以并行地处理，那么内存消耗和批量大小会成正比。硬件可能是batch size的限制因素,因此batch size是一个重要的调参对象\n",
    "\n",
    "通常是在犯错比较明显的时候学习的速度最快\n",
    "\n",
    "\n",
    "\n",
    "## Dropout\n",
    "Dropout可以被认为是集成大量深层神经网络的实用Bagging方法。Bagging涉及训练多个模型，每个测试样本上评估多个模型，Dropout提供了一种廉价版的Bagging集成。在dropout情况下，大部分模型共享参数\n",
    "\n",
    "* 在 Bagging 的情况下，所有模型都是独立的；而在 Dropout 的情况下，所有模型共享参数，其中每个模型继承父神经网络参数的不同子集。\n",
    "* 在 Bagging 的情况下，每一个模型都会在其相应训练集上训练到收敛。而在 Dropout 的情况下，通常大部分模型都没有显式地被训练；取而代之的是，在单个步骤中我们训练一小部分的子网络，参数共享会使得剩余的子网络也能有好的参数设定。\n",
    "\n",
    "\n",
    "## 参数初始化\n",
    "* 一般使用服从的高斯分布`（mean=0, stddev=1）`或均匀分布的随机值作为权重的初始化参数；使用 0 作为偏置的初始化参数\n",
    "* 一些启发式方法会根据输入与输出的单元数来决定初始值的范围 \n",
    "* 其他初始化方法\n",
    "    随机正交矩阵（Orthogonal）\n",
    "    截断高斯分布（Truncated normal distribution）\n",
    "\n",
    "\n",
    "## 为什么要归一化？？\n",
    "\n",
    " 为了后面数据处理的方便，归一化的确可以避免一些不必要的数值问题。\n",
    "    为了程序运行时收敛加快。 下面图解。\n",
    "    同一量纲。样本数据的评价标准不一样，需要对其量纲化，统一评价标准。这算是应用层面的需求。\n",
    "    避免神经元饱和。啥意思？就是当神经元的激活在接近 0 或者 1 时会饱和，在这些区域，梯度几乎为 0，这样，在反向传播过程中，局部梯度就会接近 0，这会有效地“杀死”梯度。\n",
    "    保证输出数据中数值小的不被吞食。\n",
    "    \n",
    "上图是代表数据是否均一化的最优解寻解过程（圆圈可以理解为等高线）。左图表示未经归一化操作的寻解过程，右图表示经过归一化后的寻解过程。\n",
    "\n",
    "当使用梯度下降法寻求最优解时，很有可能走“之字型”路线（垂直等高线走），从而导致需要迭代很多次才能收敛；而右图对两个原始特征进行了归一化，其对应的等高线显得很圆，在梯度下降进行求解时能较快的收敛。\n",
    "\n",
    "因此如果机器学习模型使用梯度下降法求最优解时，归一化往往非常有必要，否则很难收敛甚至不能收敛。\n",
    "\n",
    "## 卷积网络CNN\n",
    "### 卷积的动机\n",
    "卷积三大重要思想：\n",
    "* 稀疏交互(Sparse interactions)\n",
    "* 参数共享(parameter sharing)\n",
    "* 等变表示(equivariant representations)\n",
    "\n",
    "传统的神经网络中每一个输出单元与每一个输入单元都产生交互。卷积网络改进了这一点，使具有稀疏交互的特征。CNN通过核(kernel)尺寸小鱼输入的尺寸来达到这个目的\n",
    "### 稀疏带来的好处\n",
    "* 提高了模型的效率\n",
    "* 减少了模型的存储需求和计算量，如果有`m`个输入和`n`个输出，复杂度为`o(m×n)`，如果每一个输出只来自`k`个输入，那么计算复杂度为`o(k×n)`\n",
    "虽然减少了隐藏层单元之间的交互，但是实际上在**深层的单元可以间接的连接到全部或者大部分的输入(这也就是所谓的感受野)**\n",
    "### 参数共享\n",
    "参数共享是指在一个模型中的多个函数中使用相同的参数，也就是说一对输入和输出只绑定一组参数，在不同的输入节点中滑动\n",
    "### 等变/不变性\n",
    "平移不变性是一个很有用的性质，尤其是但我们关心某个特征是否出现而不太关心其出现的具体位置时。\n",
    "参数共享和(池化)使神经网络具有一定的平移不变性\n",
    ">池化操作也能够加强网络的平移不变性\n",
    "### 卷积参数计算\n",
    "#### 输出feature map尺寸\n",
    "$$\\frac{n-f+p}{s} +1$$\n",
    "n:输入尺寸 f:卷积核ch存 s:卷积步长 p:padding数量 \n",
    ">只有卷积核完全覆盖输入计算才有效，但上式算出来的商可能不是整数，惯例是向下取整\n",
    "#### 卷积conv2d以及参数量\n",
    "通常二维图像有多个通道，因此每一张图像实际上的shape是(channel,h,w)，conv2d做的其实是三维卷积，conv2d不仅仅会对二维图像输出后的尺寸变动，更重要的是在通道上的变动，在通道上多样的处理方法也衍生出了很多经典网络\n",
    "\n",
    "**conv2d权重参数量:**<br>\n",
    "$$out×in×k×k$$\n",
    "out:输出通道数量 in:输入通道数量 k:卷积核尺寸\n",
    "#### 卷积运算计算量\n",
    "计算conv2d在对输入的feature map卷积的计算量,总体思路是`输出尺寸×卷积的参数量`\n",
    "$$(\\frac{n-f+p}{s}+1)\\times out \\times in \\times k \\times k$$\n",
    ">不考虑求和和bias计算量\n",
    "### 卷积中padding\n",
    "\n",
    "1. valid卷积——不使用0填充，卷积核只允许访问图像中能够完全包含整个核的位置，输出的宽度为`n(输入图像尺寸)-f(卷积核的尺寸)+1`(假设stride=1)\n",
    "在这种情况下，输出的大小每一次都会损失一部分，(一般情况下，影响不大，除非是上百层的网络)\n",
    "2. same卷积——进行足够的0填充保持每一次输出和输入具有相同大小的尺寸。\n",
    "$p=(k-1)/2$\n",
    "p：padding填充数量  k:卷积核的尺寸\n",
    "\n",
    "\n",
    "<table style=\"width:100%; table-layout:fixed;\">\n",
    "  <tr>\n",
    "    <td><img width=\"150px\" src=\"gif/no_padding_no_strides.gif\"></td>\n",
    "    <td><img width=\"150px\" src=\"gif/arbitrary_padding_no_strides.gif\"></td>\n",
    "    <td><img width=\"150px\" src=\"gif/same_padding_no_strides.gif\"></td>\n",
    "    <td><img width=\"150px\" src=\"gif/full_padding_no_strides.gif\"></td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>无填充，步长为1</td>\n",
    "    <td>Arbitrary padding, no strides</td>\n",
    "    <td>same 模式，有填充，步长为1</td>\n",
    "    <td>Full padding, no strides</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td><img width=\"150px\" src=\"gif/no_padding_strides.gif\"></td>\n",
    "    <td><img width=\"150px\" src=\"gif/padding_strides.gif\"></td>\n",
    "    <td><img width=\"150px\" src=\"gif/padding_strides_odd.gif\"></td>\n",
    "    <td></td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>无填充，步长为2</td>\n",
    "    <td>Padding, strides</td>\n",
    "    <td>Padding, strides (odd)</td>\n",
    "    <td></td>\n",
    "  </tr>\n",
    "</table>\n",
    "\n",
    "## 反卷积(Transposed convolution) \n",
    "\n",
    "蓝色是输入的feature map, 灰绿色是输出\n",
    "\n",
    "<table style=\"width:100%; table-layout:fixed;\">\n",
    "  <tr>\n",
    "    <td><img width=\"150px\" src=\"gif/no_padding_no_strides_transposed.gif\"></td>\n",
    "    <td><img width=\"150px\" src=\"gif/arbitrary_padding_no_strides_transposed.gif\"></td>\n",
    "    <td><img width=\"150px\" src=\"gif/same_padding_no_strides_transposed.gif\"></td>\n",
    "    <td><img width=\"150px\" src=\"gif/full_padding_no_strides_transposed.gif\"></td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>No padding, no strides, transposed</td>\n",
    "    <td>Arbitrary padding, no strides, transposed</td>\n",
    "    <td>Half padding, no strides, transposed</td>\n",
    "    <td>Full padding, no strides, transposed</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td><img width=\"150px\" src=\"gif/no_padding_strides_transposed.gif\"></td>\n",
    "    <td><img width=\"150px\" src=\"gif/padding_strides_transposed.gif\"></td>\n",
    "    <td><img width=\"150px\" src=\"gif/padding_strides_odd_transposed.gif\"></td>\n",
    "    <td></td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>No padding, strides, transposed</td>\n",
    "    <td>Padding, strides, transposed</td>\n",
    "    <td>Padding, strides, transposed (odd)</td>\n",
    "    <td></td>\n",
    "  </tr>\n",
    "</table>\n",
    "\n",
    "## 空洞卷积 (Dilated convolution) \n",
    "蓝色是输入的feature map, 灰绿色是输出\n",
    "\n",
    "<table style=\"width:25%\"; table-layout:fixed;>\n",
    "  <tr>\n",
    "    <td><img width=\"150px\" src=\"gif/dilation.gif\"></td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>无填充，步长为1</td>\n",
    "  </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在pytorch中，有一个类叫`torch.nn.Parameter()`  \n",
    "该函数功能可以理解为将一个不可训练类型的`Tensor`转换为可训练的`parameter`，并将`parameter`绑定到`module`中，因此参数可以在训练的时候可以进行优化，这些参数也成了模型的一部。  \n",
    "计算网络参数量是一个常见的问题，我们常用的参数也就是通道数的"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid character in identifier (<ipython-input-2-cee7eea61fee>, line 15)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-2-cee7eea61fee>\"\u001b[0;36m, line \u001b[0;32m15\u001b[0m\n\u001b[0;31m    bn = nn.BatchNorm2d(10，affine=False)\u001b[0m\n\u001b[0m                                ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid character in identifier\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.functional as F\n",
    "import torch.nn as nn\n",
    "\n",
    "a = torch.randn((1,3,193,193))#输入尺寸(3,193,193)\n",
    "c1 = nn.Conv2d(3,10,5)#卷积k=3x3,in=3,out=10\n",
    "for i in c1.parameters():\n",
    "    print('conv2d 5x5x10 para shape:', i.shape)\n",
    "c2 = nn.Conv2d(10,10,1)\n",
    "for i in c2.parameters():\n",
    "    print('conv2d 1x1x10 para shape:', i.shape)\n",
    "m = nn.MaxPool2d(2,2)\n",
    "for i in m.parameters():#maxpooling只保存最大元素位置，不设参数\n",
    "    print('maxpooling para shape:', i.shape)\n",
    "bn = nn.BatchNorm2d(10，affine=False)\n",
    "for i in bn.parameters():\n",
    "    print('bn para shape:', i.shape)#bn的科学系\n",
    "\n",
    "TConv = nn.ConvTranspose2d(in_channels=10,out_channels=10,kernel_size=3)\n",
    "for i in TConv.parameters():\n",
    "    print('Transposed Conv:', i.shape)#Transposed Conv(转置卷积)参数量和普通的Conv是一致的\n",
    "\n",
    "Upsample = nn.Upsample(scale_factor=2, mode='bilinear')\n",
    "for i in Upsample.parameters():\n",
    "    print('Upsample :', i.shape)#Upsample不包含可学习的参数\n",
    "    \n",
    "    \n",
    "no_padding = c1(a)#经过一次5x5x10的Conv2d(no padding)\n",
    "print('output after conv1:', no_padding.shape)\n",
    "\n",
    "x = c2(no_padding)#经过一次1x1x10的Conv2d\n",
    "print('output after conv2:', x.shape)\n",
    "\n",
    "x = bn(x)\n",
    "print('output after bn:', x.shape)\n",
    "\n",
    "x = m(x)#经过一次maxpooling(stride=2)\n",
    "print('output after maxpoolig:', x.shape)# in // 2\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "通过上面可以看出，Conv2d的参数量其实为4D-Tensor，bias等于输出通道维度\n",
    "对于bn而言参数， 可学习的参数只有$\\gamma$ 和 $\\beta$， 每个通道只有"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 鉴别深度学习调包侠的试金石—BN\n",
    "据HR说能够完全回答正确BN问题的人只有少数。  \n",
    "本文全部收集来自网上，\n",
    "去任何一家公司面试深度学习相关工作，有80%的可能会被问到有关BN的知识点，据说能够完全回答正确BN问题的人只有少数，为什么面试官会如此青睐这个小小的bn呢？  \n",
    "**此处引用网友的评论：**\n",
    ">如果跑网络跑到一定程度就大概率会遇到bn的坑，遇到就会被逼着去了解bn的细节，\n",
    "所以反过来说，如果不了解bn，则大概率缺乏实践经验,\n",
    "日常使用中经常遇到bn的坑，无论是pytorch还是tf还是别的框架，如果对bn的细节不够熟悉的话连debug都无从下手\n",
    "\n",
    "本文从paper到代码梳理一下bn的前世今生  \n",
    "**Batch Normalizaiton: Accelerating Deep Network Training by Reducing Internal Covariate Shift**  \n",
    "**Author: 某某某 Christian Szegedy(谷歌大佬)**  \n",
    "* 从paper题目中就可以看出bn的作用和原理：**加速训练，减少ICS(层间分布偏移)**  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## bn参数量\n",
    "为什么要把bn的参数量单独列出来谈呢"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i in nn.MaxPool2d(2).parameters():\n",
    "    print(i.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-24-b682831cc6ba>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-24-b682831cc6ba>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    c2.4\u001b[0m\n\u001b[0m       ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "c2.4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 反卷积葬爱家族(Upsample, Transposed Conv, Dilated Conv)\n",
    "逆卷积相对于卷积在神经网络结构的正向和反向传播中做相反的运算\n",
    "不难想象就是如下的稀疏矩阵\n",
    "### 空洞卷积(Dilated Convolution)\n",
    "### 转置卷积(Transposed Convolution)\n",
    "### "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 标准神经网络\n",
    "\n",
    "代价函数关于任何一个权重的改变率：\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "复合函数求导法则\n",
    "如果函数\n",
    "$u=\\psi (t)$\n",
    "$v=\\phi (t)$\n",
    "$z=f(u,v)$在对应点$(u,v)$具有连续一阶偏导数，则复合函数$z=f[\\psi(t), \\phi(t)]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 经典CNN家族\n",
    "### LeNet5\n",
    "一种典型的用来识别数字的卷积神经网络.\n",
    "#### 模型结构\n",
    "#### 模型特性\n",
    "\n",
    "1. 卷积网络使用一个三层的序列：**卷积,池化,非线性**——这可能是自这篇论文以来面向图像的深度学习最关键的特性！！\n",
    "2. 使用卷积提取空间特征,卷积具有参数量少,稀疏性,参数共享,平移不变形\n",
    "3. 使用映射得到的空间进行降采样()\n",
    "4. tanh或sigmoid非线性激活\n",
    "5. 使用全连接层也就是机器学习中的经典MLP做最后的分类\n",
    "\n",
    "### Inception\n",
    "Inception网络是CNN\n",
    "#### Inception V4\n",
    "结合残差块可以显著加速Inception的训练。作者进一步展示了适当的激活值缩放如何稳定\n",
    "使模块更加一致。作者还注意到某些模块有不必要的复杂性。这允许我们通过添加更多的一致的模块来提高性能\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 自编码器(Autoencoder)\n",
    "自编码器是神经网络的一种，经过训练后能尝试将输入复制到输出。自编码器内部有一个隐藏层h，可以产生编码(code)用于编码输入data。该网络可以看作两部分组成，一个由函数$h=f(x)$表示编码器和一个生成重构的解码器$r=g(h)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 如何解决过拟合与欠拟合？\n",
    "**欠拟合**<br>\n",
    "1. 添加其他特征项。\n",
    "2. 添加多项式特征。例如将线性模型添加二次项或三次项使模型泛化能力更强。\n",
    "\n",
    "3. 可以增加模型的复杂程度。\n",
    "4. 减少正则化系数。正则化主要为了防止过拟合\n",
    "**过拟合：**解决过拟合最好用的方式还是要在数据上做文章<br>\n",
    "1. 重新清洗数据，数据不纯会导致过拟合，此类情况需要重新清理数据\n",
    "2. 增加训练样本数量；添加噪声数量；数据增强()\n",
    "3. 降低模型的复杂度\n",
    "4. 增大正则化系数\n",
    "5. 采用dropout方法\n",
    "6. early stoping，在val集loss开始上升的时候中断训练\n",
    "7. 减少迭代次数、\n",
    "8. 增大学习率？？\n",
    "9. 树结构中，可以对树进行剪枝\n",
    "\n",
    "## k折交叉验证\n",
    "1. 将含有N个样本的数据集，分成k份，没分含有N/K个样本。选择其中的一份作为测试集，另外K-1份作为训练集，测试集有K种情况\n",
    "2. 在每种情况中，用训练集训练模型，用测试集测试模型，计算模型的泛化误差\n",
    "3. 交叉验证重复K次，每份验证一次，平均K次的结果或者使用其他结合方式，最终得到一个单一估测，得到模型最终的泛化误差，这也就是所谓的**模型集成(model ensembling)**\n",
    "4. K折交叉验证的优势在于，同事重复运用随机产生的子样本进行训练和验证，每次的结果验证一次，10折交叉验证是最常用的\n",
    ">Note:1. 训练集中样本数量要足够多\n",
    "2.训练集和测试集必须从完整的数据集中均匀取样。均匀取样的目的是希望减少训练集、测试集与源数据集之间的偏差。当样本数量足够多时，通过随机取样，便可实现均匀取样的效果。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.列举常见的一些范数及其应用场景，如L0，L1，L2，L∞，Frobenius范数\n",
    "\n",
    "2.简单介绍一下贝叶斯概率与频率派概率，以及在统计中对于真实参数的假设。\n",
    "\n",
    "3.概率密度的万能近似器\n",
    "\n",
    "4.简单介绍一下sigmoid，relu，softplus，tanh，RBF及其应用场景\n",
    "\n",
    "5.Jacobian，Hessian矩阵及其在深度学习中的重要性\n",
    "\n",
    "6.KL散度在信息论中度量的是那个直观量\n",
    "\n",
    "7.数值计算中的计算上溢与下溢问题，如softmax中的处理方式\n",
    "\n",
    "8.与矩阵的特征值相关联的条件数(病态条件)指什么，与梯度爆炸与梯度弥散的关系\n",
    "\n",
    "9.在基于梯度的优化问题中，如何判断一个梯度为0的零界点为局部极大值／全局极小值还是鞍点，Hessian矩阵的条件数与梯度下降法的关系\n",
    "\n",
    "10.KTT方法与约束优化问题，活跃约束的定义\n",
    "\n",
    "11.模型容量，表示容量，有效容量，最优容量概念\n",
    "\n",
    "12.正则化中的权重衰减与加入先验知识在某些条件下的等价性\n",
    "\n",
    "13.高斯分布的广泛应用的缘由\n",
    "\n",
    "14.最大似然估计中最小化KL散度与最小化分布之间的交叉熵的关系\n",
    "\n",
    "15.在线性回归问题，具有高斯先验权重的MAP贝叶斯推断与权重衰减的关系，与正则化的关系\n",
    "\n",
    "16.稀疏表示，低维表示，独立表示\n",
    "\n",
    "17.列举一些无法基于地图的优化来最小化的代价函数及其具有的特点\n",
    "\n",
    "18.在深度神经网络中，引入了隐藏层，放弃了训练问题的凸性，其意义何在\n",
    "\n",
    "19.函数在某个区间的饱和与平滑性对基于梯度的学习的影响\n",
    "\n",
    "20.梯度爆炸的一些解决办法\n",
    "\n",
    "21.MLP的万能近似性质\n",
    "\n",
    "22.在前馈网络中，深度与宽度的关系及表示能力的差异\n",
    "\n",
    "23.为什么交叉熵损失可以提高具有sigmoid和softmax输出的模型的性能，而使用均方误差损失则会存在很多问题。分段线性隐藏层代替sigmoid的利弊\n",
    "\n",
    "24.表示学习的发展的初衷？并介绍其典型例子:自编码器\n",
    "\n",
    "25.在做正则化过程中，为什么只对权重做正则惩罚，而不对偏置做权重惩罚\n",
    "\n",
    "26.在深度学习神经网络中，所有的层中考虑使用相同的权重衰减的利弊\n",
    "\n",
    "27.正则化过程中，权重衰减与Hessian矩阵中特征值的一些关系，以及与梯度弥散，梯度爆炸的关系\n",
    "\n",
    "28.L1／L2正则化与高斯先验／对数先验的MAP贝叶斯推断的关系\n",
    "\n",
    "29.什么是欠约束，为什么大多数的正则化可以使欠约束下的欠定问题在迭代过程中收敛\n",
    "\n",
    "30.为什么考虑在模型训练时对输入(隐藏单元／权重)添加方差较小的噪声，与正则化的关系\n",
    "\n",
    "31.共享参数的概念及在深度学习中的广泛影响\n",
    "\n",
    "32.Dropout与Bagging集成方法的关系，以及Dropout带来的意义与其强大的原因\n",
    "\n",
    "33.批量梯度下降法更新过程中，批量的大小与各种更新的稳定性关系\n",
    "\n",
    "34.如何避免深度学习中的病态，鞍点，梯度爆炸，梯度弥散\n",
    "\n",
    "35.SGD以及学习率的选择方法，带动量的SGD对于Hessian矩阵病态条件及随机梯度方差的影响\n",
    "\n",
    "36.初始化权重过程中，权重大小在各种网络结构中的影响，以及一些初始化的方法；偏置的初始化\n",
    "\n",
    "37.自适应学习率算法:AdaGrad，RMSProp，Adam等算法的做法\n",
    "\n",
    "38.二阶近似方法:牛顿法，共轭梯度，BFGS等的做法\n",
    "\n",
    "39.Hessian的标准化对于高阶优化算法的意义\n",
    "\n",
    "40.卷积网络中的平移等变性的原因，常见的一些卷积形式\n",
    "\n",
    "41.pooling的做法的意义\n",
    "\n",
    "42.循环神经网络常见的一些依赖循环关系，常见的一些输入输出，以及对应的应用场景\n",
    "\n",
    "43.seq2seq，gru，lstm等相关的原理\n",
    "\n",
    "44.采样在深度学习中的意义\n",
    "\n",
    "45.自编码器与线性因子模型，PCA，ICA等的关系\n",
    "\n",
    "46.自编码器在深度学习中的意义，以及一些常见的变形与应用\n",
    "\n",
    "47.受限玻尔兹曼机广泛应用的原因\n",
    "\n",
    "48.稳定分布与马尔可夫链\n",
    "\n",
    "49.Gibbs采样的原理\n",
    "\n",
    "50.配分函数通常难以计算的解决方案\n",
    "\n",
    "51.几种参数估计的联系与区别:MLE／MAP／贝叶斯\n",
    "\n",
    "52.半监督的思想以及在深度学习中的应用\n",
    "\n",
    "53.举例CNN中的channel在不同数据源中的含义\n",
    "\n",
    "54.深度学习在NLP，语音，图像等领域的应用及常用的一些模型\n",
    "\n",
    "55.word2vec与glove的比较\n",
    "\n",
    "56.注意力机制在深度学习的某些场景中为何会被大量使用，其几种不同的情形\n",
    "\n",
    "57.wide&deep模型中的wide和deep介绍\n",
    "\n",
    "58.核回归与RBF网络的关系\n",
    "\n",
    "此处问题很多编者本人也只有一个来自教材书籍的局部认识，望各位批评指正，可以在评论区留下正确全面的回答，共同学习与进步。\n",
    "\n",
    "59.LSTM结构推导，为什么比RNN好？\n",
    "\n",
    "60.过拟合在深度学习中的常见的一些解决方案或结构设计\n",
    "\n",
    "61.怎么理解贝叶斯模型的有效参数数据会根据数据集的规模自动调整"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[12,  4],\n",
       "       [ 6, 12]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = np.array([[12,2],[2,3]])\n",
    "b = np.array([[1,2],[3,4]])\n",
    "a * b #普通矩阵的乘积都是按照位置进行想成，也称为Hadamard乘积"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sys\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "l = nn.Linear(2,3)\n",
    "l.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (3,5) (3,) ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-27-4b59758fe221>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtr_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m#不需要赋值，赋值之后返回NoneType！！！\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[0mnet\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNetwork\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 98\u001b[0;31m \u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSGD\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtr_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmini_batch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meta\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-27-4b59758fe221>\u001b[0m in \u001b[0;36mSGD\u001b[0;34m(self, training_data, epochs, mini_batch_size, eta, shuffle, test_data)\u001b[0m\n\u001b[1;32m     22\u001b[0m             \u001b[0mmini_batches\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtraining_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mmini_batch_size\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmini_batch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mmini_batch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmini_batches\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate_mini_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmini_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meta\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mtest_data\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m                     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Epoch {0}: {1} / {2}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-27-4b59758fe221>\u001b[0m in \u001b[0;36mupdate_mini_batch\u001b[0;34m(self, mini_batch, eta)\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[0mnabla_w\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;31m#创建存储空间防止差值\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmini_batch\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m             \u001b[0mdelta_nabla_b\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdelta_nabla_w\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackprop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m             \u001b[0mnabla_b\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mnb\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mdnb\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mnb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdnb\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnabla_b\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdelta_nabla_b\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m             \u001b[0mnabla_w\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mnw\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mdnw\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mnw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdnw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnabla_w\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdelta_nabla_w\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-27-4b59758fe221>\u001b[0m in \u001b[0;36mbackprop\u001b[0;34m(self, x, y)\u001b[0m\n\u001b[1;32m     61\u001b[0m             \u001b[0mactivations\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mactivation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;31m#计算反向传播过程\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m         \u001b[0mdelta\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcost_derivative\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mactivations\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m             \u001b[0msigmoid_prime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0mnabla_b\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdelta\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m         \u001b[0mnabla_w\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdelta\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivations\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-27-4b59758fe221>\u001b[0m in \u001b[0;36mcost_derivative\u001b[0;34m(self, output_activations, y)\u001b[0m\n\u001b[1;32m     85\u001b[0m         \u001b[0;31m\\\u001b[0m\u001b[0mpartial\u001b[0m \u001b[0ma\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mthe\u001b[0m \u001b[0moutput\u001b[0m \u001b[0mactivations\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m         \"\"\"\n\u001b[0;32m---> 87\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0moutput_activations\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0msigmoid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;36m1.0\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1.0\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mz\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: operands could not be broadcast together with shapes (3,5) (3,) "
     ]
    }
   ],
   "source": [
    "class Network:\n",
    "    def __init__(self, sizes, shuffle=True):\n",
    "        if not isinstance(sizes, list) or isinstance(sizes, tuple):\n",
    "            raise Exception('Size must be seq-like!')\n",
    "        self.num_layers = len(sizes)\n",
    "        self.sizes = sizes\n",
    "        self.biases = [np.random.randn(y, 1) for y in sizes[1:]]#偏置都是每一个Linear和out的个数\n",
    "        self.weights = [np.random.randn(y, x)#权重参数是间隔部分才有，参数量=输入x输出，\n",
    "                        for x, y in zip(sizes[:-1], sizes[1:])]#之间相加\n",
    "        self.random = shuffle\n",
    "    def feedForward(self, a):\n",
    "        for b, w in zip(self.biases, self.weights):\n",
    "            a = sigmoid(np.dot(w, a) + b)\n",
    "        return a\n",
    "    def SGD(self, training_data, epochs, mini_batch_size, eta,\n",
    "            shuffle=True, test_data=None):\n",
    "        if test_data: n_test = len(test_data)\n",
    "        n = len(training_data)\n",
    "        for j in range(epochs):\n",
    "            if self.random:\n",
    "                random.shuffle(training_data)#进行一个\n",
    "            mini_batches = [training_data[k:k+mini_batch_size] for k in range(0, n, mini_batch_size)]\n",
    "            for mini_batch in mini_batches:\n",
    "                self.update_mini_batch(mini_batch, eta)\n",
    "                if test_data:\n",
    "                    print(\"Epoch {0}: {1} / {2}\".format(j, self.evaluate(test_data), n_test))\n",
    "                else:\n",
    "                    print(\"Epoch {0} complete\".format(j))\n",
    "    def update_mini_batch(self, mini_batch, eta):\n",
    "        \"\"\"\n",
    "        Update the network's weights and biases by applying \n",
    "        gradient descent using backpropagation to a single mini batch.\n",
    "        The \"mini_batch\" is a list of tuples \"(x,y)\"\n",
    "        \"\"\"\n",
    "        nabla_b = [np.zeros(b.shape) for b in self.biases]\n",
    "        nabla_w = [np.zeros(w.shape) for w in self.weights]#创建存储空间防止差值\n",
    "        for x, y in mini_batch:\n",
    "            delta_nabla_b, delta_nabla_w = self.backprop(x, y)\n",
    "            nabla_b = [nb+dnb for nb, dnb in zip(nabla_b, delta_nabla_b)]\n",
    "            nabla_w = [nw+dnw for nw, dnw in zip(nabla_w, delta_nabla_w)]\n",
    "        self.weights = [w-(eta/len(mini_batch))*nw\n",
    "                        for w, nw in zip(self.weights, nabla_w)]\n",
    "        self.biases = [b-(eta/len(mini_batch))*nb\n",
    "                       for b, nb in zip(self.biases, nabla_b)]\n",
    "        \n",
    "    def backprop(self, x, y):\n",
    "        \"\"\"Return a tuple ``(nabla_b, nabla_w)``representing the \n",
    "        gradient for the cost function C_x. ``nabla_b`` and ``nabla_w``\n",
    "        are layer-by-layer lists of numpy arrays, similar to ``self.biases``\n",
    "        and ``self.weights``.\n",
    "        \"\"\"\n",
    "        nabla_b = [np.zeros(b.shape) for b in self.biases]\n",
    "        nabla_w = [np.zeros(w.shape) for w in self.weights]\n",
    "        activation = x#\n",
    "        activations = [x]#列出所有需要存储的中间层激活值\n",
    "        zs = []#存储每一层的z\n",
    "        for b, w in zip(self.biases, self.weights):\n",
    "            z = np.dot(w, activation) + b\n",
    "            zs.append(z)\n",
    "            activation = Network.sigmoid(z)#计算a\n",
    "            activations.append(activation)\n",
    "        #计算反向传播过程\n",
    "        delta = self.cost_derivative(activations[-1], y)* \\\n",
    "            sigmoid_prime(zs[-1])\n",
    "        nabla_b[-1] = delta\n",
    "        nabla_w[-1] = np.dot(delta, activations[-2].transpose())\n",
    "        #\n",
    "        for l in range(2, self.num_layers):\n",
    "            z = zs[-l]\n",
    "            sp = sigmoid_prime(z)\n",
    "            delta = np.dot(self.weights[-l+1].transpose(), delta) * sp#迭代进行链式连乘\n",
    "            nabla_b[-l] = delta#\n",
    "            nabla_w[-l] = np.dot(delta, activations[-l-1].transpose())\n",
    "        return (nabla_b, nabla_w)\n",
    "    def evaluate(self, test_data):\n",
    "        \"\"\"Return the number of test inputs for which the neural\n",
    "        network outputs the correct result. Note that the neural \n",
    "        network's output is assumed to be the index of whichever\n",
    "        neuron in the final layer layer has the highest activation\n",
    "        \"\"\"\n",
    "        test_results = [(np.argmax(self.feedForward(x)), y)\n",
    "                        for (x, y) in test_data]\n",
    "        return sum(int(x == y) for (x, y) in test_results)\n",
    "    def cost_derivative(self, output_activations, y):\n",
    "        \"\"\"Return the vector of partial derivatives \\partial C_x/\n",
    "        \\partial a for the output activations\n",
    "        \"\"\"\n",
    "        return (output_activations-y)\n",
    "    def sigmoid(z):\n",
    "        return 1.0 / (1.0 + np.exp(-z))\n",
    "    def sigmoid_prime(z):\n",
    "        return sigmoid(z) * (1 - sigmoid(z))\n",
    "    \n",
    "fake_input = np.random.randn(10,2)\n",
    "fake_target = np.random.randn(10,3)\n",
    "tr_data = list(zip(fake_input, fake_target))  \n",
    "random.shuffle(tr_data)#不需要赋值，赋值之后返回NoneType！！！\n",
    "net = Network([2,5,3])\n",
    "net.SGD(training_data=tr_data, epochs=30, mini_batch_size=10, eta=3 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "shapes (3,5) and (10,3) not aligned: 5 (dim 1) != 10 (dim 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-30-2f9514a9aa1a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: shapes (3,5) and (10,3) not aligned: 5 (dim 1) != 10 (dim 0)"
     ]
    }
   ],
   "source": [
    "w = np.random.randn(10,3)\n",
    "x = np.random.randn(3,5)\n",
    "y = x.dot(w)#使用dot循序，前者为前(对应矩阵乘法的规则)，不要越级\n",
    "y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\nabla$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2,)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.randn(2).shape#一维的向量的维度是(2,)，表示这是一个一维"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "w = np.random.randn(5,2)\n",
    "x = np.random.randn(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5,)\n"
     ]
    }
   ],
   "source": [
    "y = np.dot(w,x)#返回的是一维向量shape=(n,)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 5)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = np.random.randn(5,1)\n",
    "o = b + y#可以看到如果(5,)+(5,1),Numpy有返回的是一个扩维度的矩阵\n",
    "o.shape#这样会认为是一个"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2, 3, 4],\n",
       "       [2, 3, 4],\n",
       "       [2, 3, 4]])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ori = np.array([[1],[1],[1]])\n",
    "bias = np.array([1,2,3])#numpy有广播机制，可以对ori的每一列进行广播扩增，每列+bias的对应列\n",
    "ori + bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5,)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = np.random.randn(5)\n",
    "o = b + y\n",
    "o.shape#这样返回的是正常"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5,)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c = np.random.randn(5,1).squeeze()#可以通过squeeze进行压缩维度\n",
    "c.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-4.01950951e+00, -2.43988042e+00, -3.88317800e+00,\n",
       "         1.04844039e+00, -2.35706970e-01, -3.30000944e+00,\n",
       "        -1.02948988e+00, -3.54962697e-01, -1.17983548e+00,\n",
       "        -2.20876412e+00],\n",
       "       [-1.12427221e+00, -5.48668727e-01, -2.85332738e-01,\n",
       "         2.05546477e+00, -4.00807409e-01, -2.49994434e+00,\n",
       "        -1.77463045e-02, -1.12857388e-01,  4.37216344e-02,\n",
       "        -8.45705208e-01],\n",
       "       [-1.71697835e+00, -1.27972877e+00,  9.71381837e-02,\n",
       "         3.31859773e+00, -2.36665949e-01, -2.45203818e+00,\n",
       "         1.92204122e-01,  2.58986788e-01, -6.19125080e-04,\n",
       "        -2.51127355e-01],\n",
       "       [-1.28652937e+00, -2.50005218e+00,  1.84364880e+00,\n",
       "         2.94659599e+00,  1.15256377e+00,  3.03363880e+00,\n",
       "         8.78224207e-01,  1.68799747e+00, -2.67869454e-01,\n",
       "         3.43317310e+00],\n",
       "       [ 3.28290198e+00,  2.92027946e+00,  2.30548266e+00,\n",
       "        -7.56095648e-01, -6.53784061e-01, -4.77733617e-01,\n",
       "         4.68819222e-01, -5.84907385e-01,  1.19789054e+00,\n",
       "        -3.57392685e-01]])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import gzip\n",
    "import os\n",
    "def load_data(path='./minist_data/mnist.pkl.gz'):\n",
    "    if not os.path.exists(path):\n",
    "        raise OSError('Cannot find minist data')\n",
    "    f = gzip.open(path, 'rb')\n",
    "    training_data, validation_data, test_data = pickle.load(f, encoding='bytes')\n",
    "    f.close()\n",
    "    return (training_data, validation_data, test_data)\n",
    "def load_data_wrapper():\n",
    "    tr_d, va_d, te_d = load_data()\n",
    "    training_inputs = [np.reshape(x, (784, 1)) for x in tr_d[0]]\n",
    "    training_results = [vectorized_result(y) for y in tr_d[1]]\n",
    "    training_data = zip(training_inputs, training_results)\n",
    "    validation_inputs = [np.reshape(x, (784, 1)) for x in va_d[0]]\n",
    "    validation_data = zip(validation_inputs, te_d[1])\n",
    "    test_inputs = [np.reshape(x, (784, 1)) for x in te_d[0]]\n",
    "    test_data = zip(test_inputs, te_d[1])\n",
    "    return (training_data, validation_data, test_data)\n",
    "def vectorized_result(j):\n",
    "    e = np.zeros((10,1))\n",
    "    e[j] = 1.0\n",
    "    return e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tr_d, va_d, te_d = load_data_wrapper()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# net.SGD(training_data=tr_d[0], epochs=30, mini_batch_size=10, eta=3.0, test_data=te_d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 2, 3])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array([1,2,3]).transpose()#一维数组无法使用transpose，因为(1)只有一个轴，没法其他轴与其进行对换，转置还是本身，只有提升一个维度才能记性转换"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 2])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1],\n",
       "       [2]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.reshape((-1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2,)\n",
      "(3, 1)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[1, 2],\n",
       "       [2, 4],\n",
       "       [3, 6]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = np.array([1,2])\n",
    "b = np.array([1,2,3]).reshape((-1,1))\n",
    "print(a.shape)\n",
    "print(b.shape)\n",
    "a * b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[0.08448187],\n",
       "        [0.0616492 ]])]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = Network([1,2])\n",
    "net.weight"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 代码规范\n",
    "使用`import x`来导入包和模块<br>\n",
    "使用`from x import y`\n",
    "使用`from x import y as z`如果要导入的模块太长了\n",
    "导入时不要使用相对名称，即使模块在同一个包中，也要使用完整包名。这样能帮助你避免无意间导入一个包两次\n",
    "### 异常\n",
    "允许使用异常，但必须小心\n",
    "**结论：**<br>\n",
    "异常必须遵循特定条件：\n",
    "1. 像这样出发异常`raise Exception(Error message\")`或者`raise MyException`。不要使用两个参数的形式\n",
    "2. 模块或包应该定义自己的特定域异常，也不要捕获`Exception`或者`StandardError`，除非你打算重新出发异常，在异常方面，Python非常宽容，`except`真的会捕获包括Python语法错误在内的任何错误，很容易隐藏真正的bug。\n",
    "```\n",
    "class Error(Exception):\n",
    "    pass\n",
    "```\n",
    "### 函数与方法装饰器\n",
    "也就是@标记的。最常见的装饰器是@classmethod和@staticmethod，用于将常规函数转换成类方法和静态方法。不过，装饰器语法也允许用户自定义装饰器。特别的，对于某个"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
