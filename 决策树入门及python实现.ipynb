{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "决策树是一种肥惨的监督性机器学习方法。它可以用来做分类判断和回归预测。决策树的基本原理是通过学习现有的数据特征，得到简单的决策规律，再根据这些决策规律对目标进行判断。\n",
    "# 决策树的概念\n",
    "决策树是在已知各种情况发生概率的基础上，通过构成决策树来判断下一预测点，判断其是否会发生的分析方法，是只管运用概率分析的一种图解法。由于这种决策树分支的图形很像树的质感，故称为决策树。\n",
    "决策树本质上是基于过去的特征信息对过去的目标结论的判断总结，直观的解释就是：由于这件事在过去相同特征环境下大概率发生，所以此时这件事也很可能会发生，‘历史总是惊人的相似’。<br>\n",
    "但其缺点也很明显：\n",
    "1.精度较低<br>\n",
    "2.他每次只会根据单一特征划分数据，不会根据数据组合切分。<br>\n",
    "3.受误差值影响较大\n",
    "# 决策树的生成\n",
    "决策树室友根节点，分叉，叶节点组成的，决策树德生成就是寻找根节点、叶节点和如何分叉。决策树的生成核心思想就是找出更加纯净的子集，最好每个子集里都是结论及其单一的数据。判断纯度的方法不同，决策树的生成的结构自然而然也不相同，常用的判断方法有：<br>\n",
    "\n",
    "* ID3算法:使用信息增益作纯度判断\n",
    "* C4.5算法：使用信息增益率作纯度判断\n",
    "* CART算法：使用基尼系数作纯度判断\n",
    "### 生成过程\n",
    "1.寻找最合适分割的特征\n",
    "2.根据纯度判断方法，寻找最优的分割点，基于这一特征把数据分割成纯度更高的两部分数据\n",
    "3.判断是否达到要求，若未达到，重复步骤1.继续分割，知道达到要求停止\n",
    "4.剪枝，防止过拟合\n",
    "## 特征选择\n",
    "特征选择关键是选择出对训练数据具有分类能力的特征，这样可以提高决策树效率。如果利用一个特征进行分类的结果与随机分类的结果没有很大的差别，那么这个特征选取就很失败了。\n",
    "**准则：**信息增益或信息增益比\n",
    "### 信息增益\n",
    "熵(entropy)是表示随机变量不确定性的度量。设X是一个取有限个值的离散随机变量，其概率分布为 $P(X=X_i)=p_i,i=1,2,...n$\n",
    "则随机变量X熵的定义为\n",
    "$$H(X)=-\\sum _{i=1}^np_ilogp_i$$\n",
    "**熵只依赖于X的分布，而与X的取值情况无关，熵越大随机变量的不确定性就越大**\n",
    "**条件熵：**随机变量X给定条件下随机变量Y的条件熵H(Y|X)\n",
    "$$H(Y|X)=\\sum_{i=1}^np_iH(Y|X=x_i)$$\n",
    ">注意前面没有-号，因为后面有H(Y|X=x_i)了\n",
    "\n",
    "**信息增益:**特征A对训练数据集D的信息增益g(D,A),定义为集合D的经验熵H(D）与特征A给定条件下D的经验条件熵H(D|A)之差，即：\n",
    "$$g(D,A)=H(D)-H(D|A)$$\n",
    "一般地，熵H(Y)与条件熵H(Y|X)之差称为互信息。表示由于特征A而使得对数据集D分类的不确定性较少的程度。\n",
    ">此处注意信息增益与交叉熵之间的区别。交叉熵用H(p,q)表示，表示用q分布去拟合p的分布。信息增益相当于条件概率，即增加条件A后，不确定度的变化情况。信息增益只是增加了条件A，并没有引入一个全新的分布。好的特征应该使数据更趋向于一类，不确定性会减少，因此g应该为正值。\n",
    "\n",
    "### 信息增益比\n",
    "在训练数据集的经验熵打的时候，信息增益值会偏大。反之，信息增益值会偏小。使用信息增益比(information gain ratio)可能对这一问题进行校正。这是特征选择的另一准则\n",
    "**信息增益比：**特征A对训练数据集D的信息增益比g(D,A)定义为其信息增益g(D,A)与训练数据集D的经验熵H(D)之比：\n",
    "$$g(D,A)=\\frac{g(D,A)}{H(D)}$$\n",
    "\n",
    "决策树学习应用信息增益准则选择：对训练数据集(或子集)D，计算每个特征的信息增益，并比较他们的大小，选择信息增益最大的特征。\n",
    "决策树的生成\n",
    "### ID3算法\n",
    "核心是在决策树各个节点应用信息增益尊则选择特征，递归的建立决策树\n",
    "**具体方法：**从根节点开始，对节点计算所有可能的特征的信息增益，选择信息增益最大的特征作为节点的特征，由该特征的不同取值建立子节点；在对子节点递归地调用以上方法，构建决策树；直到所有特征的信息增益均很小或没有特征可以选择为止。ID3相当于用极大似然法进行概率模型的选择\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "chooseBestSplit()\n",
    "```\n",
    "对每个特征:\n",
    "    对每个特征值:\n",
    "        将数据集分成两份(小于该特征则放为左子树)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "prune()\n",
    "```\n",
    "基于已有的树切分测试数据集:\n",
    "    如果存在任一子集是一棵树,则在\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def isTree(obj):\n",
    "    return (type(obj).__name__ == '__dict__')\n",
    "\n",
    "def getMean(tree):\n",
    "    if isTree(tree['right']):\n",
    "        tree['right'] = getMean(tree['right'])\n",
    "    if isTree(tree['left']):\n",
    "        tree['left'] = getMean(tree['left'])\n",
    "    return (tree['left'] + tree['right']) / 2.0\n",
    "\n",
    "def prune(tree, testData):\n",
    "    if shape(testData)[0] == 0:\n",
    "        return getMean()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
