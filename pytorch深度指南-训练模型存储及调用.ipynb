{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pytorch网络模型存储与调用以及修改\n",
    "pytorh网络模型的存储和调用主要利用了`torch.save()`和`torch.load()`这两个函数<br>\n",
    "pytorch搭建神经网络都是按照父子网络关系生成的,通过层层调用网络的名称可以从中取出部分网络,如果继承了`nn.Module`创建的网络,需要自己手写`forward()`函数来指定网络的的层次递进关系,因为只是`__init__`定义了很多网络模块,网络之间的关系需要手工定义"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "高级一点的网络模型保存和调用可以用如下形式的函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_checkpoint(model, epoch, save_mode='generator'):\n",
    "    model_out_path = save_mode+'model/'+'model_epoch_{}.pkl'.format(epoch)\n",
    "    state = {'epoch':epoch, 'model':model}#使用字典的方式进行存储，可以将epoch编号和网络参数都存储下来\n",
    "    if not os.path.exists('model/'):#判断当前路径下文件夹是否存在，注意`model`后面要有`/`表示文件夹\n",
    "        os.makedirs('model/')\n",
    "    torch.save(state,model_out_path)\n",
    "    print('Checkpoint saved to {}'.format(model_out_path))#使用format格式化输出"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_checkpoint(epoch, model, model_path):\n",
    "    if os.path.isfile(model_path):\n",
    "        print('====>loading checkpoint:{}'.format(model_path))\n",
    "        checkpoint = torch.load(model_path)#注意要先用load加载出网络\n",
    "        start_epoch += 1\n",
    "        model.load_state_dict(checkpoint['model'].state_dict())#读取网络模型参数，注意checkpoint是用字典存储的，索引model才是之前训练的网络模型\n",
    "    print('There is no file named :{}'.format(model_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`torch.save(obj, f, pickle_module)`<br>\n",
    "**功能：**存储一个对象至本地文件,相当于把计算机内存里的对象存到本地，任意obj的类型都可以存储，load后可以继续使用。<br>\n",
    "**参数：**<br>\n",
    "-obj:待存储的对象<br>\n",
    "-f:存储的文件名，string类型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`torch.load(f, map_location=None, pickle_module)`<br>\n",
    "**功能：**读取save存储的文件，返回save相应的obj<br>\n",
    "**参数：**<br>\n",
    "f:文件名称"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`torch.nn.Module.load_state_dict(dict)`\n",
    "**功能：**网络模型载入已有的参数字典<br>\n",
    "**参数：**\n",
    "state_dict:参数字典"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`torch.nn.Module.state_dict()`<br>\n",
    "**功能：**返回包括网络所有参数的一个字典<br>\n",
    "**参数：**可以像字典一样使用,调用`.keys()`可以查看里面的键值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y tensor([[ 3.,  3.],\n",
      "        [ 3.,  3.]])\n",
      "z: tensor([[ 27.,  27.],\n",
      "        [ 27.,  27.]])\n",
      "out True\n",
      "x\n"
     ]
    }
   ],
   "source": [
    "x = torch.ones(2,2,requires_grad=True)\n",
    "y = x + 2\n",
    "print('y', y)\n",
    "z = y*y*3#tensor的乘法是对应位置相乘，不是矩阵相乘\n",
    "print('z:', z)\n",
    "out = torch.sum(z)\n",
    "print('out', out.requires_grad)\n",
    "out.backward()\n",
    "print('x', )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`nn.Sequential(*args)`里面使用的参数收集,将收集到的参数汇成一个序列,Seq生成的网络结构支持索引,即内部网络是按索引来取的,不是按照名称!!!这也体现了pytorch多样化的网络搭建方式  \n",
    "`nn.ModuleList(list)`将list中的模块搭建成一个网络,注意与Seq的区别,传入的是list  \n",
    ">这两种方式搭建的网络都是以list的感觉存在(index:对应的网络实例),传统的搭网络是以字典的方式构建(网络名称:对应的网络实例)\n",
    "这种方式有个好处可以搭建具有很多重复模块的大网络,其中很多子模块不需要给一些专有的名称,只需要使用index编号即可\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Conv2d(1, 1, kernel_size=(3, 3), stride=(1, 1))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "block = nn.Sequential(*[nn.Conv2d(1,1,3) for i in range(5)])#里面是参数收集,不是seq收集\n",
    "models = nn.ModuleList([block for i in range(10)])#初始化使用list创建\n",
    "models[0]#支持index索引\n",
    "# for i in models:#支持迭代\n",
    "#     print(i)\n",
    "models[0][0]#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'models' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-1508dd9c86c4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mConv2d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m#使用append直接添加\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mmodels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;31m#支持切片索引等骚操作\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'models' is not defined"
     ]
    }
   ],
   "source": [
    "models.append(nn.Conv2d(1,1,1))#使用append直接添加\n",
    "models[0:2]#支持切片索引等骚操作"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "l = models[0:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如果想指定每一个网络名称的具体名称,可以通过`add_module()`方式来完成,`nn.Sequential()和nn.ModuleList()`都支持这个操作,要注意层次关系,`add`进去的module是在当前list的层级,如果里面子块"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ModuleList(\n",
       "  (block1): Sequential(\n",
       "    (conv): Conv2d(1, 1, kernel_size=(3, 3), stride=(1, 1))\n",
       "    (bn): BatchNorm2d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  )\n",
       "  (block2): Sequential(\n",
       "    (conv): Conv2d(1, 1, kernel_size=(3, 3), stride=(1, 1))\n",
       "    (bn): BatchNorm2d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  )\n",
       "  (2): Sequential(\n",
       "    (conv): Conv2d(1, 1, kernel_size=(3, 3), stride=(1, 1))\n",
       "    (bn): BatchNorm2d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "block = nn.Sequential()#先使用Sequential创建一个空的Seq\n",
    "block.add_module('conv', nn.Conv2d(1,1,3))#\n",
    "block.add_module('bn', nn.BatchNorm2d(1))\n",
    "models = nn.ModuleList()\n",
    "models.add_module('block1', block)\n",
    "models.add_module('block2', block)\n",
    "models.append(block)#使用append默认使用index标号进行命名\n",
    "models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method Module.parameters of ModuleList(\n",
       "  (block1): Sequential(\n",
       "    (conv): Conv2d(1, 1, kernel_size=(3, 3), stride=(1, 1))\n",
       "    (bn): BatchNorm2d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  )\n",
       "  (block2): Sequential(\n",
       "    (conv): Conv2d(1, 1, kernel_size=(3, 3), stride=(1, 1))\n",
       "    (bn): BatchNorm2d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  )\n",
       "  (2): Sequential(\n",
       "    (conv): Conv2d(1, 1, kernel_size=(3, 3), stride=(1, 1))\n",
       "    (bn): BatchNorm2d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  )\n",
       ")>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "models.parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataParallel(\n",
       "  (module): ModuleList(\n",
       "    (block1): Sequential(\n",
       "      (conv): Conv2d(1, 1, kernel_size=(3, 3), stride=(1, 1))\n",
       "      (bn): BatchNorm2d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (block2): Sequential(\n",
       "      (conv): Conv2d(1, 1, kernel_size=(3, 3), stride=(1, 1))\n",
       "      (bn): BatchNorm2d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (2): Sequential(\n",
       "      (conv): Conv2d(1, 1, kernel_size=(3, 3), stride=(1, 1))\n",
       "      (bn): BatchNorm2d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn.DataParallel(models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 1,  2,  3,  4])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.tensor([1,2,3,4])\n",
    "F.dropout2d(a,p=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_layer = nn.Dropout2d(0.5)\n",
    "for i in drop_layer.parameters():\n",
    "    print(i.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "block1.conv.weight torch.FloatTensor\n",
      "block1.conv.bias torch.FloatTensor\n",
      "block1.bn.weight torch.FloatTensor\n",
      "block1.bn.bias torch.FloatTensor\n",
      "block1.bn.running_mean torch.FloatTensor\n",
      "block1.bn.running_var torch.FloatTensor\n",
      "block1.bn.num_batches_tracked torch.LongTensor\n",
      "block2.conv.weight torch.FloatTensor\n",
      "block2.conv.bias torch.FloatTensor\n",
      "block2.bn.weight torch.FloatTensor\n",
      "block2.bn.bias torch.FloatTensor\n",
      "block2.bn.running_mean torch.FloatTensor\n",
      "block2.bn.running_var torch.FloatTensor\n",
      "block2.bn.num_batches_tracked torch.LongTensor\n",
      "2.conv.weight torch.FloatTensor\n",
      "2.conv.bias torch.FloatTensor\n",
      "2.bn.weight torch.FloatTensor\n",
      "2.bn.bias torch.FloatTensor\n",
      "2.bn.running_mean torch.FloatTensor\n",
      "2.bn.running_var torch.FloatTensor\n",
      "2.bn.num_batches_tracked torch.LongTensor\n"
     ]
    }
   ],
   "source": [
    "for name, para in models.state_dict().items():\n",
    "    print(name, para.type())#结构是"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 网络参数的调用和修改\n",
    "在fine-tuning的时候或者对网络参数进行修改的时候，需要对网络tensor进行调整，因此需要学会对生成的网络对象进行参数读取<br>\n",
    "调用网络参数的方式有多种可以使用，但是目的只有一个，就是要访问到`tensor`级的参数，以方便我们对网络参数的修改<br>\n",
    "\n",
    "* `model.state_dict()`返回一个网络的字典,包含名称(str)和参数(nn.Parameter)\n",
    "* `model.parameters()`返回全部的网络参数的迭代器,没有名称\n",
    "* `model.modules()`返回网络全部的子模块,这个返回的是一个生成器,只有配合迭代器才有用(for循环)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TestNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(TestNet, self).__init__()\n",
    "        self.conv1 = SubNet1()\n",
    "        self.conv2 = SubNet2()\n",
    "        self.l1 = nn.Linear(10, 10)\n",
    "        self.l2 = nn.Linear(10, 15)\n",
    "        self.l3 = nn.Linear(15, 1)\n",
    "        self.test = nn.Linear(15,15)\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.l1(x)\n",
    "        x = self.l2(x)\n",
    "        out = nn.Sigmoid(self.l3(x))\n",
    "        return out\n",
    "    def set_trainable(self, layer_regex, model=None, indent=0, verbose=1):\n",
    "        for param in self.named_parameters():\n",
    "            layer_name = param[0]\n",
    "            trainable = bool(re.fullmatch(layer_regex, layer_name))\n",
    "            if not trainable:\n",
    "                param[1].requires_grad = False            \n",
    "class SubNet1(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SubNet1, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(2, 10, 1)\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        return x        \n",
    "class SubNet2(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SubNet2, self).__init__()\n",
    "        self.conv2 = nn.Conv2d(1, 1, 1)\n",
    "    def forward(self, x):\n",
    "        x = self.conv2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SubNet1(\n",
       "  (conv1): Conv2d(2, 10, kernel_size=(1, 1), stride=(1, 1))\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = TestNet()\n",
    "test.conv1#直接调用网络的属性名称可以取出部分网络"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 2, 1, 1])\n",
      "torch.Size([10])\n",
      "torch.Size([1, 1, 1, 1])\n",
      "torch.Size([1])\n",
      "torch.Size([10, 10])\n",
      "torch.Size([10])\n",
      "torch.Size([15, 10])\n",
      "torch.Size([15])\n",
      "torch.Size([1, 15])\n",
      "torch.Size([1])\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15])\n"
     ]
    }
   ],
   "source": [
    "for p in test.parameters():\n",
    "    print(p.shape)#conv2的bias的shape与out_channel一致，即每一组使用一个偏置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('', ModuleList(\n",
      "  (0): Conv2d(1, 1, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (1): Conv2d(1, 1, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (2): Conv2d(1, 1, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (3): Conv2d(1, 1, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (4): Conv2d(1, 1, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (5): Conv2d(1, 1, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (6): Conv2d(1, 1, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (7): Conv2d(1, 1, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (8): Conv2d(1, 1, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (9): Conv2d(1, 1, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (10): Conv2d(1, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "))\n",
      "('0', Conv2d(1, 1, kernel_size=(3, 3), stride=(1, 1)))\n",
      "('1', Conv2d(1, 1, kernel_size=(3, 3), stride=(1, 1)))\n",
      "('2', Conv2d(1, 1, kernel_size=(3, 3), stride=(1, 1)))\n",
      "('3', Conv2d(1, 1, kernel_size=(3, 3), stride=(1, 1)))\n",
      "('4', Conv2d(1, 1, kernel_size=(3, 3), stride=(1, 1)))\n",
      "('5', Conv2d(1, 1, kernel_size=(3, 3), stride=(1, 1)))\n",
      "('6', Conv2d(1, 1, kernel_size=(3, 3), stride=(1, 1)))\n",
      "('7', Conv2d(1, 1, kernel_size=(3, 3), stride=(1, 1)))\n",
      "('8', Conv2d(1, 1, kernel_size=(3, 3), stride=(1, 1)))\n",
      "('9', Conv2d(1, 1, kernel_size=(3, 3), stride=(1, 1)))\n",
      "('10', Conv2d(1, 1, kernel_size=(1, 1), stride=(1, 1)))\n"
     ]
    }
   ],
   "source": [
    "for i in models.named_modules():#这是一个生成器\n",
    "    print(i)\n",
    "# models[11]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prnt_weight(m):\n",
    "    print(m.weight)\n",
    "def init_weight(m):\n",
    "    print(m)\n",
    "    if type(m) == nn.Linear:\n",
    "        m.weight.data.fill_(5.0)\n",
    "net = nn.Sequential(nn.Linear(2,2), nn.Linear(2,2))\n",
    "net.apply(init_weight)\n",
    "net.apply(prnt_weight)\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net,self).__init__()\n",
    "        self.conv1 = nn.Conv2d(4,10,kernel_size=3)\n",
    "        self.fc = nn.Linear(10, 3)\n",
    "\n",
    "model = Net()\n",
    "for name, parameter in model.state_dict().items():\n",
    "    print(name, parameter.data.shape)#可以看到网络的参数shape是(input_channels*output_channels*kernel*kernel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bn = 'sub.bn'\n",
    "not 'bn' in bn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Parameter containing:\n",
       " tensor([[[[-0.2338]]],\n",
       " \n",
       " \n",
       "         [[[ 0.5962]]],\n",
       " \n",
       " \n",
       "         [[[-0.7837]]],\n",
       " \n",
       " \n",
       "         [[[ 0.7938]]],\n",
       " \n",
       " \n",
       "         [[[-0.4047]]],\n",
       " \n",
       " \n",
       "         [[[ 0.9081]]],\n",
       " \n",
       " \n",
       "         [[[-0.5584]]],\n",
       " \n",
       " \n",
       "         [[[ 0.1025]]],\n",
       " \n",
       " \n",
       "         [[[ 0.9495]]],\n",
       " \n",
       " \n",
       "         [[[-0.5842]]]]), Parameter containing:\n",
       " tensor([ 0.0888, -0.4299,  0.8976, -0.8497,  0.3468, -0.0633, -0.6725,\n",
       "         -0.6541,  0.1189, -0.2832]), Parameter containing:\n",
       " tensor([[[[ 0.8568]]]]), Parameter containing:\n",
       " tensor([-0.4776]), Parameter containing:\n",
       " tensor([[-0.3057,  0.1490, -0.0670, -0.2937,  0.2081,  0.0296,  0.1956,\n",
       "           0.1292, -0.2050, -0.0941],\n",
       "         [ 0.1719,  0.2971, -0.1711,  0.0143, -0.2868,  0.1422, -0.0949,\n",
       "          -0.1421,  0.1687, -0.0376],\n",
       "         [-0.0662,  0.0373,  0.2102, -0.1170,  0.1763, -0.2743, -0.1340,\n",
       "           0.1100,  0.1372,  0.1612],\n",
       "         [ 0.1908,  0.0613,  0.1323,  0.0355,  0.1914, -0.2417, -0.0268,\n",
       "           0.1024,  0.0134, -0.0895],\n",
       "         [ 0.0981, -0.2510, -0.0109, -0.0293, -0.1147,  0.0209,  0.0219,\n",
       "          -0.2721, -0.3040, -0.2684],\n",
       "         [ 0.2206, -0.2158,  0.2078,  0.1339, -0.2646,  0.2060,  0.1563,\n",
       "           0.2808,  0.2456,  0.2429],\n",
       "         [ 0.1509, -0.1109,  0.0443, -0.1006,  0.1194,  0.0859, -0.1329,\n",
       "           0.0562,  0.2151, -0.1655],\n",
       "         [ 0.1247,  0.1195,  0.0925, -0.0155, -0.2191,  0.0860, -0.2679,\n",
       "           0.2187, -0.2955,  0.2871],\n",
       "         [-0.0363, -0.0928, -0.0340,  0.1712, -0.1004, -0.1730, -0.2017,\n",
       "          -0.3031, -0.0670,  0.0253],\n",
       "         [-0.2733,  0.1386, -0.2357, -0.0465,  0.1605, -0.2819,  0.0593,\n",
       "           0.2194,  0.2097,  0.2388]]), Parameter containing:\n",
       " tensor([-0.2293,  0.2548,  0.0727,  0.1044, -0.2116,  0.0667,  0.3075,\n",
       "          0.1135, -0.0864,  0.1295]), Parameter containing:\n",
       " tensor([[ 0.0454, -0.1977, -0.0392,  0.2565,  0.2393, -0.2882,  0.0617,\n",
       "           0.0733,  0.0788, -0.2270],\n",
       "         [ 0.0187, -0.1384,  0.2287,  0.0235,  0.0086,  0.1373,  0.0779,\n",
       "          -0.2269, -0.2281, -0.1759],\n",
       "         [-0.2503, -0.2911,  0.1654,  0.1317,  0.0540,  0.1883, -0.2008,\n",
       "          -0.0416, -0.2601, -0.3013],\n",
       "         [ 0.3106, -0.2865, -0.2384, -0.2767, -0.0377,  0.0651,  0.2483,\n",
       "           0.0212, -0.1148,  0.0939],\n",
       "         [-0.2302,  0.1606,  0.2800, -0.0603,  0.2308,  0.3141, -0.1672,\n",
       "           0.0278,  0.1240, -0.1187],\n",
       "         [ 0.0493, -0.0201,  0.0671, -0.0038, -0.0310,  0.1666,  0.1183,\n",
       "           0.2748, -0.1744,  0.0883],\n",
       "         [-0.1656, -0.2539, -0.0239, -0.0841,  0.0799, -0.2430,  0.1910,\n",
       "          -0.2244, -0.1571, -0.0291],\n",
       "         [ 0.2939, -0.2892,  0.2803, -0.2538, -0.2764, -0.2850, -0.1034,\n",
       "          -0.1325,  0.2749, -0.2327],\n",
       "         [-0.3099,  0.1134, -0.0359,  0.2721,  0.0678, -0.3012,  0.1630,\n",
       "           0.2836,  0.0317, -0.2114],\n",
       "         [ 0.1273, -0.2954,  0.0665,  0.0843,  0.2208,  0.1566,  0.2482,\n",
       "          -0.3118,  0.0939, -0.0980],\n",
       "         [-0.0995,  0.1472,  0.0738,  0.2383, -0.0161,  0.2824, -0.2681,\n",
       "          -0.0690, -0.1385, -0.0794],\n",
       "         [ 0.1854,  0.2774, -0.0596, -0.0350, -0.2583, -0.0493,  0.2175,\n",
       "          -0.1582, -0.2818, -0.2767],\n",
       "         [ 0.3155,  0.2242,  0.1436,  0.2432,  0.2289, -0.0424, -0.2885,\n",
       "          -0.2177,  0.2332,  0.0343],\n",
       "         [ 0.2345,  0.1677, -0.2190, -0.2524, -0.3090, -0.0275,  0.1946,\n",
       "           0.1833,  0.0876,  0.1554],\n",
       "         [ 0.1839, -0.0592,  0.1896,  0.2364,  0.1819,  0.1751,  0.2850,\n",
       "          -0.0025,  0.3142, -0.1601]]), Parameter containing:\n",
       " tensor([-0.1021,  0.0382, -0.2895, -0.0100,  0.1479, -0.2292, -0.2730,\n",
       "          0.1657, -0.3013,  0.0417,  0.0635,  0.1340,  0.2989,  0.0277,\n",
       "          0.2968]), Parameter containing:\n",
       " tensor([[ 0.2226, -0.1130, -0.2229, -0.0231, -0.2547,  0.0152, -0.0568,\n",
       "          -0.0061, -0.2478, -0.0071,  0.0080, -0.2287, -0.0013,  0.1205,\n",
       "          -0.0488]]), Parameter containing:\n",
       " tensor([-0.1252]), Parameter containing:\n",
       " tensor([[-0.1770,  0.1976,  0.1675,  0.2259,  0.1520,  0.2083,  0.2377,\n",
       "          -0.0285, -0.0277, -0.0292,  0.1271,  0.1246,  0.0896, -0.1329,\n",
       "           0.1462],\n",
       "         [-0.0223, -0.0595, -0.0786,  0.1963, -0.1782, -0.1958, -0.1546,\n",
       "          -0.1403, -0.1962, -0.2488, -0.1811, -0.1493, -0.0140, -0.1306,\n",
       "           0.0155],\n",
       "         [ 0.0061,  0.1690, -0.0175,  0.0404, -0.0107, -0.1331, -0.2229,\n",
       "           0.0237,  0.2251, -0.1806,  0.2298, -0.1656, -0.1657, -0.1564,\n",
       "           0.0451],\n",
       "         [-0.1511, -0.0969, -0.1367,  0.1314, -0.2143, -0.2262, -0.0525,\n",
       "           0.1381, -0.2104, -0.1756, -0.0387,  0.1571, -0.0118, -0.0855,\n",
       "          -0.0210],\n",
       "         [ 0.2235, -0.1311, -0.0394, -0.1368, -0.0166,  0.2536,  0.0962,\n",
       "          -0.1478,  0.2202, -0.0017, -0.0439,  0.2496,  0.1702, -0.0906,\n",
       "          -0.0882],\n",
       "         [-0.2031,  0.0551, -0.1424,  0.1021,  0.2171,  0.2575,  0.2273,\n",
       "           0.1743,  0.1234, -0.0710,  0.1559, -0.2351,  0.2107, -0.2169,\n",
       "           0.1046],\n",
       "         [ 0.0001,  0.2372,  0.2308, -0.2129,  0.1657, -0.2488,  0.0198,\n",
       "           0.2050,  0.1175,  0.2419,  0.2106,  0.2290, -0.1946, -0.1082,\n",
       "          -0.2192],\n",
       "         [ 0.1746,  0.1048, -0.1716, -0.0318, -0.2331, -0.0246, -0.1654,\n",
       "           0.1873, -0.2533, -0.0646, -0.1863,  0.0160,  0.0867,  0.2013,\n",
       "          -0.1335],\n",
       "         [-0.0353,  0.0720,  0.1305,  0.2103, -0.1594, -0.0536,  0.2496,\n",
       "           0.1137,  0.1620,  0.1466, -0.0755, -0.0711,  0.0542,  0.2329,\n",
       "          -0.1659],\n",
       "         [ 0.1705,  0.0613,  0.0981, -0.1269, -0.2158,  0.1941, -0.1595,\n",
       "           0.1834,  0.2036,  0.2126, -0.2217,  0.1773,  0.0172, -0.0001,\n",
       "          -0.1176],\n",
       "         [ 0.0139, -0.0304,  0.0530, -0.0664,  0.2422, -0.0697,  0.2074,\n",
       "          -0.2180, -0.1851, -0.0529,  0.1400, -0.1213, -0.2351, -0.1968,\n",
       "          -0.0854],\n",
       "         [-0.0184,  0.1339,  0.1572,  0.2478,  0.1371,  0.2105, -0.0395,\n",
       "          -0.1524, -0.1739,  0.2051, -0.0275,  0.0679,  0.2237, -0.0167,\n",
       "           0.1789],\n",
       "         [-0.1872,  0.2479,  0.0182,  0.0526, -0.2137, -0.2090,  0.0555,\n",
       "          -0.0885,  0.1957,  0.0652,  0.1603,  0.2406, -0.1492, -0.1206,\n",
       "          -0.2283],\n",
       "         [ 0.0305, -0.2491,  0.2443, -0.0951,  0.0251,  0.0797,  0.0425,\n",
       "          -0.1945, -0.1872, -0.2342,  0.0824,  0.1505,  0.0439, -0.1170,\n",
       "          -0.0056],\n",
       "         [ 0.0122, -0.1994,  0.2155,  0.1223,  0.2145,  0.0358,  0.0426,\n",
       "           0.1057, -0.0420, -0.2021,  0.1820, -0.1010, -0.1074, -0.0675,\n",
       "          -0.0934]]), Parameter containing:\n",
       " tensor([ 0.2085,  0.1220,  0.1276, -0.1441, -0.1849,  0.2582, -0.1997,\n",
       "         -0.0227, -0.1057,  0.2217,  0.1360, -0.0971, -0.0815, -0.0378,\n",
       "         -0.2061])]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainable_en = [para for i,para in net.named_parameters() if para.requires_grad and not 'bn' in name]\n",
    "trainable_en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current name: conv1.conv1.weight False\n",
      "current name: conv1.conv1.bias False\n",
      "current name: conv2.conv2.weight False\n",
      "current name: conv2.conv2.bias False\n",
      "current name: l1.weight False\n",
      "current name: l1.bias False\n",
      "current name: l2.weight False\n",
      "current name: l2.bias False\n",
      "current name: l3.weight False\n",
      "current name: l3.bias False\n",
      "current name: test.weight False\n",
      "current name: test.bias False\n"
     ]
    }
   ],
   "source": [
    "layer_regex = {\n",
    "    # all layers but the backbone\n",
    "    \"heads\": r\"(fpn.P5\\_.*)|(fpn.P4\\_.*)|(fpn.P3\\_.*)|(fpn.P2\\_.*)|(rpn.*)|(classifier.*)|(mask.*)\",\n",
    "    # From a specific Resnet stage and up\n",
    "    \"3+\": r\"(fpn.C3.*)|(fpn.C4.*)|(fpn.C5.*)|(fpn.P5\\_.*)|(fpn.P4\\_.*)|(fpn.P3\\_.*)|(fpn.P2\\_.*)|(rpn.*)|(classifier.*)|(mask.*)\",\n",
    "    \"4+\": r\"(fpn.C4.*)|(fpn.C5.*)|(fpn.P5\\_.*)|(fpn.P4\\_.*)|(fpn.P3\\_.*)|(fpn.P2\\_.*)|(rpn.*)|(classifier.*)|(mask.*)\",\n",
    "    \"5+\": r\"(fpn.C5.*)|(fpn.P5\\_.*)|(fpn.P4\\_.*)|(fpn.P3\\_.*)|(fpn.P2\\_.*)|(rpn.*)|(classifier.*)|(mask.*)\",\n",
    "    # All layers\n",
    "    \"all\": \".*\",\n",
    "}\n",
    "net = TestNet()\n",
    "for name, para in net.named_parameters():#named_parameters()返回名称和具体的参数\n",
    "    print('current name:', name,bool(re.fullmatch(r'.* ', name)))#那么属于匹配项中的规则,返回除"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'TestNet' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-a1c1b24b0446>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnet\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTestNet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mparameter\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparameter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mparameter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequires_grad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;31m#可以对网络参数requires_grad属性进行调整\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# for sub_module in net.modules():\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'TestNet' is not defined"
     ]
    }
   ],
   "source": [
    "net = TestNet()\n",
    "for parameter in net.parameters():\n",
    "    print(parameter)\n",
    "    parameter.requires_grad = False#可以对网络参数requires_grad属性进行调整\n",
    "# for sub_module in net.modules():\n",
    "#     if isinstance(sub_module, nn.Linear):\n",
    "#         print('name:', sub_module,'shape:', sub_module.weight.shape, sub_module.weight)\n",
    "net.add_module('fc', nn.Linear(1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv1.conv1.weight tensor([[[[ 0.1985]]],\n",
      "\n",
      "\n",
      "        [[[ 0.5570]]],\n",
      "\n",
      "\n",
      "        [[[ 0.8402]]],\n",
      "\n",
      "\n",
      "        [[[ 0.4672]]],\n",
      "\n",
      "\n",
      "        [[[-0.0484]]],\n",
      "\n",
      "\n",
      "        [[[ 0.4602]]],\n",
      "\n",
      "\n",
      "        [[[ 0.5147]]],\n",
      "\n",
      "\n",
      "        [[[ 0.1821]]],\n",
      "\n",
      "\n",
      "        [[[-0.3727]]],\n",
      "\n",
      "\n",
      "        [[[ 0.4286]]]])\n",
      "conv1.conv1.bias tensor([ 0.5193, -0.1868, -0.0874,  0.9345,  0.1159,  0.1085, -0.3076,\n",
      "        -0.6507,  0.1185,  0.9971])\n",
      "conv2.conv2.weight tensor([[[[-0.4924]]]])\n",
      "conv2.conv2.bias tensor(1.00000e-02 *\n",
      "       [ 2.8391])\n",
      "l1.weight tensor([[-0.1799,  0.2904, -0.2168,  0.0447,  0.2767, -0.3133, -0.2258,\n",
      "          0.2662,  0.2113,  0.1514],\n",
      "        [-0.1426, -0.2526,  0.1498, -0.2656,  0.0142,  0.2884, -0.2284,\n",
      "         -0.0431,  0.2485, -0.2345],\n",
      "        [-0.1643,  0.1828, -0.2477,  0.0220, -0.1337,  0.0651, -0.3099,\n",
      "         -0.1271,  0.2799, -0.2499],\n",
      "        [-0.2457, -0.0152,  0.0334,  0.2132,  0.1115,  0.0639,  0.0008,\n",
      "         -0.2417, -0.0906,  0.1119],\n",
      "        [-0.2372,  0.0252,  0.1404, -0.2443,  0.0907, -0.1240,  0.0251,\n",
      "          0.0881,  0.2029,  0.1265],\n",
      "        [ 0.1892,  0.1264, -0.2928,  0.0357, -0.1711,  0.3072,  0.1018,\n",
      "         -0.0476, -0.2891, -0.2544],\n",
      "        [ 0.1759, -0.1752, -0.2485, -0.1074,  0.2238, -0.2639, -0.1158,\n",
      "         -0.1060, -0.0706,  0.1912],\n",
      "        [-0.1563, -0.1768, -0.2211,  0.2049,  0.0458,  0.1359, -0.2346,\n",
      "         -0.2156, -0.1888,  0.1078],\n",
      "        [-0.2632, -0.2524, -0.1976,  0.1859, -0.0302,  0.2368,  0.1438,\n",
      "          0.2214, -0.2938,  0.1066],\n",
      "        [-0.2355, -0.0893, -0.0584, -0.2647, -0.1158, -0.2866, -0.1832,\n",
      "          0.3028,  0.1773,  0.3109]])\n",
      "l1.bias tensor([-0.1248, -0.0126, -0.2330,  0.2658,  0.0450, -0.0571, -0.2536,\n",
      "         0.0459,  0.0779,  0.2768])\n",
      "l2.weight tensor([[-0.1663,  0.3110,  0.2696, -0.2268, -0.0870,  0.2720,  0.1901,\n",
      "         -0.2039,  0.1723,  0.0903],\n",
      "        [ 0.0646,  0.2114,  0.0172,  0.2187,  0.2029,  0.2724, -0.2178,\n",
      "          0.0175, -0.0311,  0.1153],\n",
      "        [ 0.1411, -0.1523, -0.2525, -0.1612, -0.3145, -0.0549,  0.2840,\n",
      "         -0.1457, -0.0986, -0.0208],\n",
      "        [-0.1818, -0.2178,  0.1991, -0.1162, -0.1831, -0.1530,  0.0447,\n",
      "         -0.0459, -0.1418, -0.1162],\n",
      "        [-0.2223, -0.0749, -0.1379,  0.3072, -0.2113, -0.1479,  0.0915,\n",
      "         -0.0273,  0.0657, -0.3106],\n",
      "        [-0.2845,  0.0503,  0.1540, -0.2066, -0.2251, -0.1585, -0.1342,\n",
      "          0.1429, -0.2179,  0.1920],\n",
      "        [-0.1902, -0.1601,  0.1339, -0.0220,  0.3152,  0.2412, -0.0664,\n",
      "         -0.2396,  0.0269,  0.3063],\n",
      "        [-0.1735,  0.1197, -0.1297, -0.2294,  0.0796,  0.0697,  0.2763,\n",
      "         -0.0714, -0.2396, -0.0771],\n",
      "        [-0.2656,  0.1798, -0.0379,  0.2919, -0.2005, -0.1373,  0.2839,\n",
      "          0.2998, -0.2160,  0.0030],\n",
      "        [-0.0124, -0.1734,  0.2036, -0.2427, -0.2334,  0.2667, -0.0480,\n",
      "          0.0562,  0.2242, -0.2729],\n",
      "        [ 0.0767, -0.3017, -0.2145, -0.2349,  0.0368,  0.2690, -0.0943,\n",
      "          0.0265,  0.1022,  0.0318],\n",
      "        [ 0.1116,  0.0690, -0.1930, -0.0933, -0.1849, -0.2774,  0.1209,\n",
      "          0.1468,  0.3112, -0.1062],\n",
      "        [ 0.1170,  0.2653, -0.1944, -0.0038, -0.1238, -0.0273,  0.1817,\n",
      "          0.0427,  0.2178,  0.3148],\n",
      "        [-0.2718,  0.1593,  0.0753, -0.0403, -0.3038,  0.0768, -0.3151,\n",
      "          0.0787,  0.0460,  0.1517],\n",
      "        [ 0.1103,  0.3001, -0.3087, -0.3014,  0.0455, -0.0708,  0.2983,\n",
      "         -0.1917, -0.2920,  0.1553]])\n",
      "l2.bias tensor([-0.1908, -0.0813,  0.2017, -0.0100, -0.1486, -0.2315, -0.2103,\n",
      "         0.2955, -0.0582, -0.0205,  0.0012, -0.1887, -0.1339, -0.2499,\n",
      "         0.3080])\n",
      "l3.weight tensor([[-0.1162, -0.0555,  0.2171,  0.2357,  0.0263,  0.0977, -0.2037,\n",
      "          0.0443, -0.2028,  0.0353, -0.1665,  0.0360,  0.2051, -0.1120,\n",
      "          0.0867]])\n",
      "l3.bias tensor([ 0.2091])\n",
      "test.weight tensor([[-0.1468, -0.1899, -0.1543, -0.2497,  0.1443, -0.2504,  0.0215,\n",
      "          0.1635,  0.1689, -0.2526, -0.0926,  0.2539,  0.2517, -0.0510,\n",
      "         -0.1456],\n",
      "        [ 0.2078, -0.0618,  0.1117,  0.0759, -0.0202,  0.1008, -0.1166,\n",
      "         -0.0799, -0.2176, -0.1503, -0.0014,  0.1520, -0.0643,  0.0610,\n",
      "          0.0025],\n",
      "        [ 0.1603,  0.2363,  0.1002, -0.1950,  0.1807, -0.2542, -0.2423,\n",
      "          0.0542, -0.0499,  0.0808,  0.1064,  0.0287,  0.0411, -0.1158,\n",
      "         -0.0347],\n",
      "        [ 0.1839,  0.1130,  0.1408, -0.0904, -0.0007,  0.2236, -0.0018,\n",
      "          0.1983, -0.1297,  0.1227, -0.2200,  0.2390, -0.1315,  0.2540,\n",
      "         -0.0832],\n",
      "        [-0.2228,  0.0515,  0.2579, -0.1765,  0.2151,  0.0531,  0.1280,\n",
      "         -0.1770,  0.2232, -0.0249, -0.1466,  0.2001,  0.2409,  0.1386,\n",
      "         -0.2349],\n",
      "        [-0.0640, -0.0767, -0.1049,  0.1982,  0.2483,  0.1200, -0.2507,\n",
      "         -0.0026, -0.1478, -0.1926,  0.1075,  0.1875,  0.1502,  0.0465,\n",
      "          0.1237],\n",
      "        [-0.1468, -0.1646,  0.2079, -0.2532, -0.1926, -0.1568,  0.0698,\n",
      "          0.2414,  0.1626, -0.1436,  0.0689, -0.0843, -0.1465, -0.0705,\n",
      "          0.0008],\n",
      "        [-0.0820,  0.2210, -0.0126,  0.1017,  0.1461, -0.0387, -0.2321,\n",
      "          0.0686, -0.2245,  0.1804, -0.0935,  0.0546, -0.2559,  0.2332,\n",
      "          0.1612],\n",
      "        [ 0.2486,  0.1742,  0.1361,  0.2436, -0.0132,  0.1265,  0.1111,\n",
      "         -0.2048,  0.0456, -0.2099,  0.0345,  0.2038, -0.2551,  0.2553,\n",
      "          0.0761],\n",
      "        [ 0.2454,  0.0773, -0.0370, -0.0206, -0.1091,  0.1157,  0.2015,\n",
      "         -0.0476, -0.0697,  0.0120, -0.0506,  0.1349,  0.1114,  0.0112,\n",
      "         -0.0141],\n",
      "        [ 0.0824, -0.2223, -0.1954,  0.1227, -0.2231,  0.1158,  0.1441,\n",
      "         -0.1400, -0.0212,  0.0528, -0.2447, -0.1020,  0.1900, -0.1662,\n",
      "          0.0658],\n",
      "        [-0.2133, -0.2098, -0.1655, -0.1017,  0.1490, -0.0594,  0.1166,\n",
      "          0.1857,  0.2081, -0.0271, -0.2572,  0.2041,  0.0361,  0.0211,\n",
      "          0.1638],\n",
      "        [ 0.2558, -0.0884, -0.0498, -0.0853, -0.1737,  0.2528, -0.1809,\n",
      "          0.0695, -0.0766,  0.1418, -0.0298,  0.1444,  0.0692,  0.1249,\n",
      "         -0.0133],\n",
      "        [-0.1739, -0.1630,  0.1499,  0.2304,  0.2099, -0.0421, -0.0518,\n",
      "         -0.2580, -0.0006,  0.0107, -0.2487, -0.0327,  0.1678, -0.0978,\n",
      "          0.2444],\n",
      "        [-0.0465, -0.1515, -0.1529,  0.0451,  0.1645,  0.0994,  0.2528,\n",
      "          0.0363, -0.2289, -0.0507,  0.0006,  0.2204, -0.1274, -0.2168,\n",
      "         -0.1060]])\n",
      "test.bias tensor([-0.1630, -0.1325,  0.0162, -0.0363,  0.0557, -0.1469,  0.2511,\n",
      "         0.0033, -0.0720,  0.2080, -0.1076,  0.1074,  0.1636, -0.2371,\n",
      "         0.1147])\n",
      "fc.weight tensor([[ 0.9929]])\n",
      "fc.bias tensor([-0.1173])\n"
     ]
    }
   ],
   "source": [
    "for name, parameter in net.state_dict().items():#\n",
    "    print('name:'name, 'parameter', parameter)#使用这种方式可以按照key读取每一个模块的参数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "注意，在网络模型类中定义的子网络无论是否在forward中调用，都会被保存在对象中，只是说如果使用forward进行反向传播，会对forward指定的网络参数进行优化调整，没有被forward的参数不会发生变化而已。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Net' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-043f66381a0d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparameter\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparameter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m#可以看到网络的参数shape是(input_channels*output_channels*kernel*kernel)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Net' is not defined"
     ]
    }
   ],
   "source": [
    "model = Net()\n",
    "for name, parameter in model.state_dict().items():\n",
    "    print(name, parameter.data.shape)#可以看到Conv2d网络的参数shape是(input_channels*output_channels*kernel*kernel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape: torch.Size([10, 1, 1, 1]) <class 'torch.nn.parameter.Parameter'>\n",
      "shape: torch.Size([10]) <class 'torch.nn.parameter.Parameter'>\n",
      "shape: torch.Size([1, 1, 1, 1]) <class 'torch.nn.parameter.Parameter'>\n",
      "shape: torch.Size([1]) <class 'torch.nn.parameter.Parameter'>\n",
      "shape: torch.Size([10, 10]) <class 'torch.nn.parameter.Parameter'>\n",
      "shape: torch.Size([10]) <class 'torch.nn.parameter.Parameter'>\n",
      "shape: torch.Size([15, 10]) <class 'torch.nn.parameter.Parameter'>\n",
      "shape: torch.Size([15]) <class 'torch.nn.parameter.Parameter'>\n",
      "shape: torch.Size([1, 15]) <class 'torch.nn.parameter.Parameter'>\n",
      "shape: torch.Size([1]) <class 'torch.nn.parameter.Parameter'>\n",
      "shape: torch.Size([15, 15]) <class 'torch.nn.parameter.Parameter'>\n",
      "shape: torch.Size([15]) <class 'torch.nn.parameter.Parameter'>\n",
      "shape: torch.Size([1, 1]) <class 'torch.nn.parameter.Parameter'>\n",
      "shape: torch.Size([1]) <class 'torch.nn.parameter.Parameter'>\n",
      "type: <class 'torch.Tensor'> tensor data: tensor([[[[ 0.7657]]],\n",
      "\n",
      "\n",
      "        [[[ 0.8929]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0256]]],\n",
      "\n",
      "\n",
      "        [[[-0.0868]]],\n",
      "\n",
      "\n",
      "        [[[-0.7924]]],\n",
      "\n",
      "\n",
      "        [[[ 0.3751]]],\n",
      "\n",
      "\n",
      "        [[[ 0.9173]]],\n",
      "\n",
      "\n",
      "        [[[ 0.7509]]],\n",
      "\n",
      "\n",
      "        [[[-0.2320]]],\n",
      "\n",
      "\n",
      "        [[[-0.6915]]]])\n",
      "type: <class 'torch.Tensor'> tensor data: tensor([ 0.4701,  0.2965,  0.6854,  0.0264, -0.3976, -0.0044, -0.8990,\n",
      "         0.7513,  0.5215, -0.5505])\n",
      "type: <class 'torch.Tensor'> tensor data: tensor([[[[ 0.5746]]]])\n",
      "type: <class 'torch.Tensor'> tensor data: tensor([-0.4814])\n",
      "type: <class 'torch.Tensor'> tensor data: tensor([[-0.1236, -0.3134,  0.1021, -0.0704, -0.1691, -0.2175, -0.1994,\n",
      "         -0.2873, -0.1857, -0.0558],\n",
      "        [ 0.0372,  0.3109,  0.0516, -0.0244,  0.0537, -0.0763,  0.2932,\n",
      "          0.2352,  0.1731, -0.2912],\n",
      "        [ 0.1781, -0.2604,  0.0738,  0.2016, -0.2822, -0.3109,  0.1169,\n",
      "         -0.0657,  0.2318, -0.1721],\n",
      "        [-0.1925,  0.0387,  0.3127, -0.0995,  0.2481, -0.1579,  0.1672,\n",
      "          0.2458, -0.2259,  0.0852],\n",
      "        [ 0.1781,  0.0933, -0.2984, -0.0636,  0.1413,  0.0186,  0.2232,\n",
      "         -0.0578, -0.0840,  0.1960],\n",
      "        [ 0.0780,  0.1226,  0.3129,  0.1746, -0.0887,  0.0567, -0.1004,\n",
      "         -0.1433,  0.0686,  0.1257],\n",
      "        [-0.0383,  0.2757, -0.0238, -0.1011, -0.0665, -0.0244,  0.2053,\n",
      "         -0.1763, -0.2946,  0.1786],\n",
      "        [-0.1763,  0.2959,  0.1305, -0.1434, -0.1628,  0.1873,  0.0213,\n",
      "         -0.0294, -0.0566, -0.0699],\n",
      "        [ 0.1847, -0.2965,  0.0351,  0.1064, -0.0099,  0.3002,  0.1861,\n",
      "          0.1539, -0.0953,  0.0566],\n",
      "        [ 0.1877,  0.0327, -0.1540,  0.1737, -0.1187, -0.0623, -0.3070,\n",
      "         -0.2051, -0.2483,  0.0351]])\n",
      "type: <class 'torch.Tensor'> tensor data: tensor([ 0.1682,  0.0004, -0.1447,  0.1838,  0.2337, -0.0821, -0.1169,\n",
      "         0.0845,  0.0786, -0.1942])\n",
      "type: <class 'torch.Tensor'> tensor data: tensor([[-0.0355,  0.0712,  0.0086, -0.2039,  0.2266, -0.0801, -0.0603,\n",
      "         -0.1968,  0.1894, -0.1807],\n",
      "        [-0.1858, -0.2203, -0.1050,  0.2997,  0.2222, -0.2908,  0.2021,\n",
      "          0.2000,  0.0167, -0.0392],\n",
      "        [ 0.1935, -0.2946, -0.0422,  0.1238,  0.1663,  0.3031, -0.0840,\n",
      "          0.2770, -0.2567,  0.1492],\n",
      "        [ 0.2424, -0.2576,  0.0200, -0.2451,  0.3122, -0.1904,  0.1057,\n",
      "          0.0679, -0.2764, -0.0363],\n",
      "        [ 0.2057, -0.0600, -0.0470,  0.0973,  0.1049,  0.3021,  0.3067,\n",
      "          0.3125,  0.1509, -0.0840],\n",
      "        [-0.1709,  0.2985, -0.0971, -0.1873, -0.0818, -0.1635,  0.2500,\n",
      "          0.0520, -0.0325, -0.2084],\n",
      "        [ 0.2109,  0.1622,  0.2043, -0.1940, -0.1876, -0.2890,  0.1909,\n",
      "          0.1077,  0.0598,  0.2735],\n",
      "        [-0.1696, -0.2599,  0.0194, -0.0022, -0.0980, -0.1679,  0.0229,\n",
      "          0.1030, -0.1069, -0.1004],\n",
      "        [-0.1347, -0.0995, -0.1828, -0.3010,  0.1530,  0.2635, -0.1770,\n",
      "         -0.1248,  0.2083, -0.1708],\n",
      "        [ 0.2748,  0.1133,  0.1138,  0.1247,  0.1254,  0.1535,  0.1952,\n",
      "          0.2790,  0.2969,  0.2504],\n",
      "        [ 0.0313,  0.1545, -0.1976,  0.1232, -0.1492,  0.1696,  0.0099,\n",
      "          0.0422, -0.1446, -0.2610],\n",
      "        [-0.2083, -0.2864, -0.1679, -0.1775, -0.0352,  0.1574,  0.1904,\n",
      "         -0.1931, -0.0714,  0.2051],\n",
      "        [ 0.2001,  0.1796,  0.0394,  0.0014,  0.0346,  0.2048,  0.1983,\n",
      "         -0.2570,  0.0613, -0.1066],\n",
      "        [ 0.3152, -0.1402,  0.0466,  0.1976, -0.1015,  0.0670,  0.0027,\n",
      "         -0.0175, -0.0639, -0.2034],\n",
      "        [-0.2224,  0.2349,  0.1844, -0.0074, -0.0096, -0.2822,  0.2064,\n",
      "          0.2372,  0.2331,  0.0164]])\n",
      "type: <class 'torch.Tensor'> tensor data: tensor([-0.2429,  0.0052, -0.0636, -0.2941, -0.2864,  0.1487,  0.1653,\n",
      "         0.2577, -0.1994,  0.2794, -0.2652,  0.0970,  0.1392, -0.0583,\n",
      "         0.2960])\n",
      "type: <class 'torch.Tensor'> tensor data: tensor([[-0.1372,  0.0863, -0.2121, -0.2058,  0.0727, -0.2000,  0.1682,\n",
      "          0.1900,  0.0734, -0.1994, -0.1258, -0.0859,  0.1191, -0.1195,\n",
      "         -0.1476]])\n",
      "type: <class 'torch.Tensor'> tensor data: tensor(1.00000e-02 *\n",
      "       [ 4.8930])\n",
      "type: <class 'torch.Tensor'> tensor data: tensor([[-0.2172,  0.0788, -0.1022,  0.2165, -0.1435, -0.1307, -0.1599,\n",
      "          0.1646,  0.0962, -0.2165,  0.0554,  0.0453,  0.2480,  0.1374,\n",
      "          0.2102],\n",
      "        [-0.0221,  0.2441, -0.0756, -0.0512, -0.2176, -0.1948, -0.0072,\n",
      "         -0.0057, -0.1234, -0.0903,  0.1216,  0.1368, -0.0327, -0.1922,\n",
      "          0.1994],\n",
      "        [ 0.1928,  0.2206,  0.0850,  0.2341,  0.1851,  0.2418,  0.2277,\n",
      "          0.1454, -0.0625,  0.2277,  0.1156,  0.2415, -0.0627, -0.2450,\n",
      "          0.1385],\n",
      "        [-0.0399,  0.0670, -0.1315, -0.2368, -0.0922,  0.0100, -0.2140,\n",
      "         -0.1606,  0.1039,  0.1025, -0.0047,  0.1442,  0.0953,  0.2364,\n",
      "         -0.0410],\n",
      "        [-0.0193, -0.2102,  0.0869, -0.0251, -0.2373, -0.1174, -0.0971,\n",
      "         -0.0541, -0.1844,  0.2201,  0.0635, -0.1265,  0.0474, -0.1796,\n",
      "          0.0573],\n",
      "        [-0.1207,  0.0407, -0.2479,  0.0355, -0.0500, -0.2143,  0.0875,\n",
      "          0.0066, -0.1005,  0.1617, -0.0414, -0.2396, -0.0158,  0.0242,\n",
      "         -0.2510],\n",
      "        [-0.2074, -0.1048,  0.0129, -0.1498,  0.0211,  0.0264,  0.2051,\n",
      "          0.0568,  0.1018, -0.2082,  0.1198, -0.0607, -0.0697, -0.0648,\n",
      "         -0.0222],\n",
      "        [ 0.2287,  0.0225,  0.0245,  0.0633,  0.0527, -0.1325, -0.0533,\n",
      "         -0.2059,  0.0984, -0.1022, -0.0493,  0.0286, -0.0267,  0.1981,\n",
      "         -0.2519],\n",
      "        [-0.0351, -0.0793,  0.2495, -0.0950, -0.0552, -0.1461, -0.2366,\n",
      "          0.1074, -0.2202,  0.0517, -0.0584, -0.0038,  0.2356,  0.2221,\n",
      "          0.0522],\n",
      "        [ 0.0690,  0.0393,  0.1343,  0.0247,  0.0386,  0.1386, -0.0706,\n",
      "         -0.1512, -0.0838,  0.2404,  0.0217, -0.1844, -0.1715,  0.1899,\n",
      "         -0.1891],\n",
      "        [-0.2165,  0.1698,  0.0130,  0.2059, -0.1949, -0.0362, -0.1429,\n",
      "         -0.1292,  0.0589, -0.0125,  0.0014, -0.1422,  0.1079,  0.2016,\n",
      "         -0.2187],\n",
      "        [-0.2567,  0.1256,  0.1097,  0.1433, -0.0768, -0.2121,  0.0112,\n",
      "         -0.1471, -0.2460, -0.2217, -0.1255, -0.0759, -0.0665,  0.0240,\n",
      "          0.2449],\n",
      "        [-0.2438,  0.1379, -0.0730,  0.1825,  0.0566,  0.0184,  0.0006,\n",
      "          0.0884,  0.1633,  0.0371, -0.1131, -0.2288, -0.0890,  0.2318,\n",
      "         -0.1435],\n",
      "        [ 0.1062,  0.0565, -0.0278, -0.0316, -0.1466,  0.1704,  0.2141,\n",
      "          0.2330,  0.0739,  0.0886,  0.1084,  0.1569,  0.0774,  0.0087,\n",
      "         -0.1896],\n",
      "        [ 0.1183, -0.0178,  0.1675, -0.2476,  0.2079,  0.2089,  0.1486,\n",
      "          0.0378, -0.1927, -0.0124, -0.1476,  0.1412,  0.0980, -0.0371,\n",
      "         -0.2284]])\n",
      "type: <class 'torch.Tensor'> tensor data: tensor([-0.1811,  0.1231, -0.0934,  0.0677, -0.0706, -0.0968,  0.2188,\n",
      "        -0.2497,  0.1504, -0.1587,  0.1298, -0.2443,  0.0677, -0.1753,\n",
      "         0.1151])\n",
      "type: <class 'torch.Tensor'> tensor data: tensor([[ 0.6638]])\n",
      "type: <class 'torch.Tensor'> tensor data: tensor([-0.9739])\n"
     ]
    }
   ],
   "source": [
    "for i in net.parameters():\n",
    "    print('shape:',i.size(), type(i))#可以看到对继承nn.Module类来讲，使用parameter()方法获取所有的数据,\n",
    "for i in net.parameters():\n",
    "    print('type:',type(i.data), 'tensor data:', i.data)#对parameter.data属性返回的才是tensor类型，之后可以调用tensor类的一些属性和方"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'net' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-12e42cef05c5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;31m#使用字典形式索引\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'fc.weight'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;31m#state_dict()中存的是网络的名称和数据参数\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'net' is not defined"
     ]
    }
   ],
   "source": [
    "for i in net.state_dict():#使用字典形式索引\n",
    "    print(i)\n",
    "net.state_dict()['fc.weight']#state_dict()中存的是网络的名称和数据参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TestNet(\n",
      "  (conv1): SubNet1(\n",
      "    (conv1): Conv2d(1, 10, kernel_size=(1, 1), stride=(1, 1))\n",
      "  )\n",
      "  (conv2): SubNet2(\n",
      "    (conv2): Conv2d(1, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "  )\n",
      "  (l1): Linear(in_features=10, out_features=10, bias=True)\n",
      "  (l2): Linear(in_features=10, out_features=15, bias=True)\n",
      "  (l3): Linear(in_features=15, out_features=1, bias=True)\n",
      "  (test): Linear(in_features=15, out_features=15, bias=True)\n",
      "  (fc): Linear(in_features=1, out_features=1, bias=True)\n",
      ")\n",
      "SubNet1(\n",
      "  (conv1): Conv2d(1, 10, kernel_size=(1, 1), stride=(1, 1))\n",
      ")\n",
      "Conv2d(1, 10, kernel_size=(1, 1), stride=(1, 1))\n",
      "SubNet2(\n",
      "  (conv2): Conv2d(1, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      ")\n",
      "Conv2d(1, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "Linear(in_features=10, out_features=10, bias=True)\n",
      "Linear(in_features=10, out_features=15, bias=True)\n",
      "Linear(in_features=15, out_features=1, bias=True)\n",
      "Linear(in_features=15, out_features=15, bias=True)\n",
      "Linear(in_features=1, out_features=1, bias=True)\n"
     ]
    }
   ],
   "source": [
    "for i in net.modules():\n",
    "    print(i)#可以看到在打印的时候会先打印整个网络的架构，然后对自定义网络模块进行解析打印，最后开内部实际的网络结构"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.nn.modules.conv.Conv2d'>\n",
      "<class 'torch.nn.parameter.Parameter'>\n"
     ]
    }
   ],
   "source": [
    "for i in net.modules():\n",
    "    if isinstance(i, nn.Conv2d):\n",
    "        i.weight.data.fill_(0)\n",
    "        print(type(i))#\n",
    "#         i.weight.data.copy_()\n",
    "print(type(net.conv.conv1.weight))#如果使用了子网络嵌套模式，索引名称是一个嵌套形式"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">注意：继承`nn.Module`的模块类型是没有weight属性的，但是通过module()返回的每个子模块有weight和bias属性，里面保存了神经网络的参数\n",
    "nn.Module是没有weight属性的，里面每一个子modlue是有weight属性的，weight和bias是对应参数，使用model.parameters()返回一个可迭代的parameter列表,里面"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "当访问到tensor级的参数时，就相当方便了，不仅可以使用tensor内置的方法和属性，也可以使用其他的工具包进行处理。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Linear(in_features=1, out_features=1, bias=True)\n",
       "  (1): Linear(in_features=1, out_features=1, bias=True)\n",
       "  (2): Linear(in_features=1, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq = torch.nn.Sequential(*[nn.Linear(1,1) for i in range(3)])\n",
    "seq.cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 网络输出类型转换tensor转numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.27922434], dtype=float32)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out = seq(torch.tensor([1.]).cuda())\n",
    "out.cpu().data.numpy()#首先,不能直接numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "不能讲输出的tensor直接转换为numpy,需要使用.data进行调用,相当于把纯数字的部分(不包含梯度)取出来才可以\n",
    "* 此外,如果是cuda类型的数据,首先是需要转换为cpu()才可以正常调用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TestNet(\n",
       "  (fc1): Linear(in_features=10, out_features=20, bias=True)\n",
       "  (dropLayer): Dropout2d(p=0.5)\n",
       "  (fc2): Linear(in_features=20, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class TestNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(TestNet, self).__init__()\n",
    "        self.fc1 = nn.Linear(10,20)\n",
    "        self.dropLayer = nn.Dropout2d(0.5)\n",
    "        self.fc2 = nn.Linear(20,1)\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.dropLayer(x)\n",
    "        return self.fc2(x)\n",
    "\n",
    "model = TestNet()\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.4621,  0.8525,  0.0080,  0.4500,  0.0087,  0.2150,  0.7558,\n",
       "          0.7200,  0.5114,  0.3416]])"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.rand(10).unsqueeze(0)#送入网络的维度要高一维\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TestNet(\n",
       "  (fc1): Linear(in_features=10, out_features=20, bias=True)\n",
       "  (dropLayer): Dropout2d(p=0.5)\n",
       "  (fc2): Linear(in_features=20, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 254,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.1677]])"
      ]
     },
     "execution_count": 296,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()#通过设置eval,dropout的随机性会消失,再次设置为train打开\n",
    "model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regex = r\".*/\\w+(\\d{4})(\\d{2})(\\d{2})T(\\d{2})(\\d{2})/mask\\_rcnn\\_\\w+(\\d{4})\\.pth\"#########正则表达式\n",
    "m = re.match(regex, model_path)\n",
    "if m:\n",
    "    now = datetime.datetime(int(m.group(1)), int(m.group(2)), int(m.group(3)),\n",
    "                            int(m.group(4)), int(m.group(5)))\n",
    "    self.epoch = int(m.group(6))\n",
    "\n",
    "# Directory for training logs\n",
    "self.log_dir = os.path.join(self.model_dir, \"{}{:%Y%m%dT%H%M}\".format(\n",
    "self.config.NAME.lower(), now))\n",
    "\n",
    "# Path to save after each epoch. Include placeholders that get filled by Keras.\n",
    "self.checkpoint_path = os.path.join(self.log_dir, \"mask_rcnn_{}_*epoch*.pth\".format(\n",
    "self.config.NAME.lower()))\n",
    "self.checkpoint_path = self.checkpoint_path.replace(\n",
    "\"*epoch*\", \"{:04d}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
