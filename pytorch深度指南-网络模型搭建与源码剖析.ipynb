{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def apply(self, fn):#多叉树式递归查找\n",
    "    for module in self.children():\n",
    "        module.apply(fn)#递归入口,相当于\n",
    "    for param in self.parameters():\n",
    "        if param is not None:\n",
    "            param.data = fn(param.data)\n",
    "            if param.grad is not None:\n",
    "                param.grad.data = fn(param.grad.data)\n",
    "def train(self, mode=True):#此处设为True\n",
    "    self.training = mode#实际执行的函数,相当于将每个叶子节点training标志设为True\n",
    "    for child in self.children():\n",
    "        child.train(mode)#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Module 源代码\n",
    "下面是pytorch中的源代码，我们对其中部分内容进行了注释"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "class Module(object):\n",
    "    def __init__(self):\n",
    "        self._parameters = OrderedDict()\n",
    "        self.training = True\n",
    "        self._modules = OrderedDict()#初始化的模块全部以字典的形式存储,变量声明为protected类型\n",
    "        self._buffers = OrderedDict()\n",
    "        self._backward_hooks = OrderedDict()\n",
    "        self._forward_pre_hooks = OrderedDict()\n",
    "    def resiger_buffer(self, name, tensor):\n",
    "        if not isinstance(name, torch._six.string_classes):\n",
    "            raise TypeError(\"Buffer name should be a string\")\n",
    "        elif '.' in name:\n",
    "            raise KeyError(\"Buffer name can't be empty\")\n",
    "        else:\n",
    "            self._buffers[name] = tensor\n",
    "    def register_parameter(self, name, param):\n",
    "        if '_parameters' not in self.__dict__:#self.__dict__判断名称是否在self的变量中\n",
    "            raise AttributeError(#善用raise提出error\n",
    "            \"cannot assign parameter before Module.__init__() call\")\n",
    "        elif not isinstance(name, torch._six.string_classes):\n",
    "            raise \n",
    "    def cuda(self, device=None):\n",
    "        return self._apply(lambda t: t.cuda(device))\n",
    "    def add_module(self, name, module):\n",
    "        if not isinstance(module, Module) and module is not None:#判断添加的是否Module对象\n",
    "            raise TypeError('Error')\n",
    "        elif not isinstance(name, torch._six.string_classes):\n",
    "            raise TypeError(\"module name cannot contain \\\".\\\"\")\n",
    "        self._modules[name] = module\n",
    "    def _apply(self, fn):\n",
    "        for module in self.children():#给子Module应用函数,递归的调用,相当于多叉树,先进性位置定位,使用for循环递归式遍历\n",
    "            module._apply(fn)\n",
    "        for param in self._parameters.values():\n",
    "            if param is not None:\n",
    "                param.data = fn(param.data)\n",
    "                if param._grad is not None:\n",
    "                    param._grad.data = fn(param._grad.data)\n",
    "    def apply(self, fn):\n",
    "        for module in self.children():\n",
    "            module.apply(fn)\n",
    "        fn(self)\n",
    "        return self\n",
    "    \n",
    "    def children(self):\n",
    "        for name, module in self.named_children():#使用生成器的方式逐个产出数据\n",
    "            yield module\n",
    "    def named_children(self):\n",
    "        memo = set()\n",
    "        for name, module in self._modules.items():#对字典进行遍历,构造一个生成器,yield返回\n",
    "            if module is not None and module not in memo:\n",
    "                memo.add(module)\n",
    "                yield name, module\n",
    "    def modules(self):\n",
    "        r\"\"\"\n",
    "        返回一个所有模块的迭代器\n",
    "        \"\"\"\n",
    "        for name, module in self.named_modules():\n",
    "            yield module\n",
    "    def named_modules(self, memo=None, prefix=''):\n",
    "        if memeo is None:\n",
    "            memo = set()\n",
    "        if self not in memo:\n",
    "            memo.add(self)\n",
    "            yield prefix, self\n",
    "            for name, module in self._modules.items():\n",
    "                if module is None:\n",
    "                    continue\n",
    "                submodule_prefix = prefix + ('.' if prefix else '') + name\n",
    "                for m in module.named_modules(memo, submodule_prefix):\n",
    "                    yield m\n",
    "    def train(self, mode=True):\n",
    "        r\"\"\"\n",
    "        只对某几层模型有效:Dropout, BatchNorm\n",
    "        \"\"\"\n",
    "        self.training = mode\n",
    "        for module in self.children():\n",
    "            module.train(mode)\n",
    "    def eval(self):\n",
    "        return self.train(False)\n",
    "    def zero_grad(self):\n",
    "        for p in self.parameters():\n",
    "            if p.grad is not None:\n",
    "                p.grad.detach_()#分离出某个节点,使其运算不会影响梯度\n",
    "                p.grad.zero_()\n",
    "    def forward(self, *input):\n",
    "        raise NotImplementedError\n",
    "    def __call__(self, *input, **kwargs):#在call中会调用forward函数,因此model的call传入了实参,和forward的形参保持一致\n",
    "        for hook in self._forward_pre_hooks.values():\n",
    "            hook(self, input)\n",
    "        if torch._c._get_tracing_state():\n",
    "            result = self._slow_forward(*input, **kwargs)\n",
    "        else:\n",
    "            result = self.for\n",
    "    def register_forward_pre_hook(self, hook):\n",
    "        handle = hooks.RemovableHandle(self._forward_pre_hooks)\n",
    "        self._forward_pre_hooks[handle.id] = hook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class A:\n",
    "    def __init__(self):\n",
    "        print('I am a initer')\n",
    "    def __call__(self, *input):\n",
    "        retult = self.forward(*input)\n",
    "    def forward(self, v1, v2):\n",
    "        print(v1, v2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
