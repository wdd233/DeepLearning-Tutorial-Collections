{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`torch.Tensor`会继承某些`torch`的某些数学运算，例如sort, min/max....不需要调用相应的`torch.funciton`进行处理,下文中如果是`torch/Tensor`即表示该函数可以直接对self的tensor使用，也可以使用torch给的相应函数接口"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `torch/Tensor.reshape(input, shape) → Tensor` \n",
    "指定tensor新的shape，reshape过后不会更该数据量和数据格式，只是对数据的shape做了调整,因此要保证reshape前后元素个数一致。\n",
    "**参数：**\n",
    "\n",
    "    input(tensor) - 输入tensor\n",
    "    shape(tuple or *size) - 新的shape \n",
    "    如果还剩下一个维度，很好，你可以直接使用-1来替代，表示这个维度中应该有的元素数量\n",
    "## `torch/Tensor.view()`\n",
    "改变形状，\n",
    "rned tensor will be a view of input. Otherwise, it will be a copy. Contiguous inputs and inputs with compatible strides can be reshaped without copying, but you should not depend on the copying vs. viewing behavior.\n",
    "\n",
    ">二者区别：当tensor都为contiguous类型时，两个函数并无差异，使用原来的数据内容，不会复制一份新的出来；"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.,  1.,  2.,  3.,  4.],\n",
       "        [ 5.,  6.,  7.,  8.,  9.]])"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.arange(10).reshape((2,5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.,  1.],\n",
       "        [ 2.,  3.],\n",
       "        [ 4.,  5.],\n",
       "        [ 6.,  7.],\n",
       "        [ 8.,  9.]])"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.arange(10).reshape(5, -1)#可以看到新的shape是(5,2),使用-1自动求剩余最后一个维度"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `torch.index_select(input, dim, index, out=None)`\n",
    "指定在哪个轴上，index是多少，注意index要是一个tensor，\n",
    "dim和index确定了一个维度的坐标$(dim_1=index_i,....dim_n=index)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.0334,  0.9123, -1.2300, -0.9336],\n",
      "        [-0.8364,  0.6584,  0.6878, -2.5896],\n",
      "        [ 0.1862, -0.3752,  0.4150, -1.4008]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.1862, -0.3752],\n",
       "        [ 0.4150, -1.4008]])"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.randn([3,4])\n",
    "print(a)\n",
    "a.index_select(0,torch.tensor([2])).reshape(2,2)#\n",
    "# torch.index_select"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `torch.linespace(start, end, steps=100, out=None)`\n",
    "返回一个一维tensor，包含**step**个元素，每个元素之间等间距，依次递增$\\frac{end-start}{step}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.0000,  0.1111,  0.2222,  0.3333,  0.4444,  0.5556,  0.6667,\n",
       "         0.7778,  0.8889,  1.0000])"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.linspace(0,1,10, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `Tensor.repeat(input, *size)`\n",
    "把输入的input当做一个基本模块m，扩张成*size的tensor，其中每个元素为m，最后返回的Tensor的shape为(*)\n",
    "*size必须比input的维度要高，\n",
    "\n",
    "```\n",
    "if input和size维度相等：\n",
    "input = (a,b)\n",
    "*size = (c,d)\n",
    "out = (a×c, b×d)\n",
    "\n",
    "size > input\n",
    "input = (a,b)\n",
    "*size = (c,d,e)\n",
    "out = (c, d×a,e×b)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 4, 6])"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.Tensor.repeat(torch.tensor([2,3]),2,4,3).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `torch/Tensor.nonzero()`\n",
    "返回一个包含输入input中非零元素索引的张量，输出张量中非零元素的索引\n",
    "若输入input有`n`维，则输出的索引张量output形状为`z * n`, 这里z是输入张量input中所有非零元素的个数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a: tensor([[ 1.,  0.,  0.],\n",
      "        [ 0.,  1.,  0.],\n",
      "        [ 0.,  0.,  1.]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ 0,  0],\n",
       "        [ 1,  1],\n",
       "        [ 2,  2]])"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.eye(3,3)\n",
    "print('a:', a)\n",
    "a.nonzero()#所有非0的坐标，注意返回的每个元素都是一个完整的坐标index～"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b: tensor([[[ 0.,  1.],\n",
      "         [ 2.,  3.]],\n",
      "\n",
      "        [[ 4.,  5.],\n",
      "         [ 6.,  7.]]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ 0,  0,  1],\n",
       "        [ 0,  1,  0],\n",
       "        [ 0,  1,  1],\n",
       "        [ 1,  0,  0],\n",
       "        [ 1,  0,  1],\n",
       "        [ 1,  1,  0],\n",
       "        [ 1,  1,  1]])"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = torch.arange(8).reshape(2,2,2)\n",
    "print('b:', b)\n",
    "b.nonzero()#z=7,n=3,输出(7*3)矩阵"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `torch/Tensor.chunk(tensor, chunks, dim=0)`\n",
    "将一个tensor在指定维度上分成chunks个数据块，为cat的逆过程,最后一个块可能会小一些，返回的是一个元组，每个元素都是其中一块\n",
    "**参数:**\n",
    "\n",
    "    tensor (Tensor) – 输入Tensor\n",
    "    chunks (int) – 分成几块的数量\n",
    "    dim (int) – 沿着哪个维度进行切分\n",
    ">可以看成`torch.cat()`的逆运算过程"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 0.0722,  0.2573, -0.2504,  1.7426],\n",
       "         [-3.3098,  0.8971,  0.0274, -0.2365]]),\n",
       " tensor([[ 1.1137,  0.3401, -1.4004,  1.2823],\n",
       "         [-0.5553,  0.8056, -0.1764,  0.7666]]),\n",
       " tensor([[ 1.3739, -1.4938,  1.8446,  0.4783],\n",
       "         [-1.5898, -0.2520, -0.4873,  1.6098]]),\n",
       " tensor([[ 0.5421, -0.2846,  0.1441, -0.5456]]))"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c = torch.randn(7,4)\n",
    "c.chunk(chunks=4,dim=0)#，在dim=0上划分，返回的是一个元组，共分为4块"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `torch/Tensor.clamp(input, min, max, out=None)`\n",
    "\n",
    "功能：将tensor所有元素裁剪到指定范围[min, max]并返回\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 3,  3,  3,  4,  5,  6,  6,  6])"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c = torch.tensor([1,2,3,4,5,6,7,8])\n",
    "c = c.clamp(3,6)#裁剪并返回\n",
    "c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `torch/Tensor.sort(input, dim=None, descending=False, out=None)`\n",
    "沿着指定维度排序,如果没有指定维度，默认最后一个维度,descending选择升序还是降序,返回一个元组(排序好的tensor和相应的索引)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0,  9,  7,  6,  3,  0],\n",
       "        [ 8,  1,  8,  3,  6,  4]], dtype=torch.int32)"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.randint(0,10,(2,6)).squeeze().int()\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 0,  1,  7,  3,  3,  0],\n",
       "         [ 8,  9,  8,  6,  6,  4]], dtype=torch.int32),\n",
       " tensor([[ 0,  1,  0,  1,  0,  0],\n",
       "         [ 1,  0,  1,  0,  1,  1]]))"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.sort(dim=0)#返回排序好的值和索引"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `torch/Tensor.topk(input, k, dim=None, largest=True, sorted=True, out=None) -> (Tensor, LongTensor)`\n",
    "返回在指定维度上k个最大的元素，默认最后一个维度，返回值是一个元组包括值和索引(values, indices)\n",
    "**参数：**\n",
    "\n",
    "    input (Tensor) – 输入tensor\n",
    "    k (int) – k个最值元素\n",
    "    dim (int) – 沿着哪个维度排许\n",
    "    largest (bool) – 选取最大元素还是最小元素\n",
    "    sorted (bool) – 返回元素是否有序"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([ 2.0778,  1.6199,  0.8411,  0.2411]), tensor([ 0,  1,  9,  4]))"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.randn(12).topk(4)#返回topK数值和位置索引"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `torch.split(tensor, split_size_or_sections, dim=0)`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `torch/Tensor.flatten(input, start_dim=0, end_dim=-1) → Tensor`\n",
    "将一个tensor按照存储位置顺序展平\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-1.6724, -0.0280,  0.0100],\n",
      "        [ 0.3437,  0.9472, -0.3655],\n",
      "        [-0.3134, -2.6223, -0.7097]])\n"
     ]
    }
   ],
   "source": [
    "a = torch.randn(3,3)\n",
    "print(a)\n",
    "# torch.flatten(a)\n",
    "# a.flatten()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `torch.numel()`\n",
    "返回tensor包含所有元素的数量"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `torch.stack(seq, dim=0, out=None) → Tensor`\n",
    "\n",
    "在一个新的维度上，拼接原有的tensor,注意该操作会产生新的维度，待组合的tensor要有相同的size\n",
    "\n",
    "**参数：**\n",
    "\n",
    "    seq (sequence of Tensors) – 待拼接的tensor，要以seq形式\n",
    "    dim (int) – dimension to insert. Has to be between 0 and the number of dimensions of concatenated tensors (inclusive)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `torch/Tensor.squeeze(input, dim=None, out=None) → Tensor`\n",
    "去除tensor中维度=1所在的那个维度，很多时候，为了保持tensor的shape对齐会提升维度，提升一些无关紧要的1维,然而在使用的时候我们不希望这些无关紧要的维度干扰我们的操作。\n",
    "\n",
    "例如，如果输出shape：(A×1×B×1×D),经过squeeze后(A×B×D)\n",
    ">注意：返回的tensor与输入共享存储空间，改变其中任何一个都会改变相应其他那个\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a: tensor([[[[ 0.,  1.,  2.,  3.,  4.],\n",
      "          [ 5.,  6.,  7.,  8.,  9.]]]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[[  2.,   3.,   4.,   5.,   6.],\n",
       "          [  7.,   8.,   9.,  10.,  11.]]]])"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.arange(10).reshape(1,1,2,5)\n",
    "print('a:', a)\n",
    "b = a.squeeze() \n",
    "b += 2\n",
    "a#可以看到虽然对b操作，但是原始a的值也发生了变化"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `torch/Tensor.unsqueeze()`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `torch.transpose(input, dim0, dim1) → Tensor`\n",
    "轴/坐标索引 交换\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `torch.meshgrid(*tensors, **kwargs)`\n",
    "    Take NNN tensors, each of which can be either scalar or 1-dimensional vector, and create NNN N-dimensional grids, where the :math:`i`th grid is defined by expanding the :math:`i`th input over dimensions defined by other inputs.\n",
    "创建坐标系\n",
    "        Args:\n",
    "            tensors (list of Tensor): list of scalars or 1 dimensional tensors. Scalars will be treated as tensors of size (1,)(1,)(1,) automatically\n",
    "        Returns:\n",
    "            seq (sequence of Tensors): If the input has kkk tensors of size (N1,),(N2,),…,(Nk,)(N_1,), (N_2,), \\ldots , (N_k,)(N1​,),(N2​,),…,(Nk​,), then the output would also has kkk tensors, where all tensors are of size (N1,N2,…,Nk)(N_1, N_2, \\ldots , N_k)(N1​,N2​,…,Nk​).\n",
    "\n",
    "        Example:\n",
    "\n",
    "        >>> x = torch.tensor([1, 2, 3])\n",
    "        >>> y = torch.tensor([4, 5, 6])\n",
    "        >>> grid_x, grid_y = torch.meshgrid(x, y)\n",
    "        >>> grid_x\n",
    "        tensor([[1, 1, 1],\n",
    "                [2, 2, 2],\n",
    "                [3, 3, 3]])\n",
    "        >>> grid_y\n",
    "        tensor([[4, 5, 6],\n",
    "                [4, 5, 6],\n",
    "                [4, 5, 6]])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `torch.where(condition, x, y) → Tensor`\n",
    "逐个位置元素判断，返回一个tensor可能来自源数据x或者y。\n",
    "\n",
    "    $$outi={xiif conditioniyiotherwiseout_i = \\begin{cases} x_i & \\text{if } \\text{condition}_i \\\\ y_i & \\text{otherwise} \\\\ \\end{cases} outi​={xi​yi​​if conditioni​otherwise​$$\n",
    "\n",
    "\n",
    "    **注意：**\n",
    "    条件,x,y都必须是可广播的\n",
    "    **参数:**\t\n",
    "    \n",
    "        condition (ByteTensor) – When True (nonzero), yield x, otherwise yield y\n",
    "        x (Tensor) – values selected at indices where condition is True\n",
    "        y (Tensor) – values selected at indices where condition is False\n",
    "\n",
    "    Returns:\t\n",
    "\n",
    "    返回的tensor的shape与条件,x和y都是一致的！！)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x: tensor([[ 0.8968,  3.3511],\n",
      "        [-0.8421, -1.1878],\n",
      "        [-0.0903,  0.6168]])\n",
      "y tensor([[ 1.,  1.],\n",
      "        [ 1.,  1.],\n",
      "        [ 1.,  1.]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.8968,  3.3511],\n",
       "        [ 1.0000,  1.0000],\n",
       "        [ 1.0000,  0.6168]])"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.randn(3,2)\n",
    "y = torch.ones(3,2)\n",
    "print('x:', x)\n",
    "print('y', y)\n",
    "torch.where(x > 0 , x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `Tensor.type(torch.type)`tensor数据类型转换\n",
    "`Tensor.int()`将数据转为int\n",
    "\n",
    "`Tensor.long()`将数据转为longInt\n",
    "\n",
    "`Tensor.byte()`将数据转换为无符号整型\n",
    "\n",
    "`Tensor.float()`将数据转换为float类型\n",
    "\n",
    "`Tensor.double()`将数据转换为double类型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'torch.LongTensor'"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.tensor([1,2,3,4])\n",
    "a.long().type()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'torch.FloatTensor'"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.float().type()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'torch.ByteTensor'"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.byte().type()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `Tensor.cuda(device, non_blocking=False)`\n",
    "Returns a copy of this object in CUDA memory.\n",
    "返回一个CUDA内存数据的副本,将输入\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.arange(1200, dtype=torch.float64).cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `Tensor.clone() `\n",
    "Returns a copy of the self tensor. The copy has the same size and data type as self.\n",
    "返回`self`tensor的一个拷贝，与原数据有相同的size和数据类型\n",
    ">clone()得到的Tensor不仅拷贝了原始的value，而且会计算梯度传播信息，copy_()只拷贝数值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a_grad: tensor([ 0.4000,  0.8000,  1.2000,  1.6000,  2.0000])\n"
     ]
    }
   ],
   "source": [
    "a = torch.tensor([1,2,3,4,5], dtype=torch.float32, requires_grad=True)\n",
    "b = a.clone()#a经过克隆得到b，\n",
    "c = (b ** 2).mean()#\n",
    "c.backward()\n",
    "print('a_grad:', a.grad)#但是梯度传播没有因此中断"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `Tensor.copy_(src, non_blocking=False)`\n",
    "只拷贝`src`的数据到`self`tensor，并返回`self`，`src`和`self`可以有不同的数据类型和不同的设备上.<br>\n",
    "**参数：**\n",
    "\n",
    "    src (Tensor) – 源数据\n",
    "    non_blocking (bool) – 如果是True，copy操作跨CPU和GPU，但可能会出现异步问题"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a_grad: tensor([ 0.4000,  0.8000,  1.2000,  1.6000,  2.0000])\n",
      "c_grad: None\n"
     ]
    }
   ],
   "source": [
    "a = torch.tensor([1,2,3,4,5], dtype=torch.float32, requires_grad=True)\n",
    "b = (a ** 2).mean()\n",
    "b.backward()\n",
    "print('a_grad:', a.grad)\n",
    "c = torch.zeros(5)\n",
    "c.copy_(a)\n",
    "print('c_grad:', c.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `Tensor.contiguous()`\n",
    "将tensor改为连续存储模式"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `Tensor.fill_()`内容填充\n",
    "`Tensor.fill_(value)`tensor内部全部填充value元素\n",
    "\n",
    "`Tensor.zero_()`\n",
    "\n",
    "`Tensor.normal_(mean, std, out=None)`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 3.,  3.,  3.],\n",
       "        [ 3.,  3.,  3.],\n",
       "        [ 3.,  3.,  3.]])"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.randn(3,3)\n",
    "a.fill_(3)#指定内容填充\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0.]])"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.zero_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.1044, -0.1353, -0.7540],\n",
       "        [-0.6852,  0.2683,  0.8297],\n",
       "        [ 2.8920,  0.2846, -1.1742]])"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.normal_()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `torch`生成常用矩阵\n",
    "### `torch.zeros(*size)`\n",
    "生成全零矩阵\n",
    "### `torch.ones(*size)`\n",
    "生成全1矩阵\n",
    "### `torch.eye(*size)`\n",
    "生成对角矩阵，对角线元素全为1\n",
    "### `torch.arange(start=0, end, step=1, out=None, dtype=None, layout=torch.strided, device=None, requires_grad=False)`\n",
    "返回指定区间，指定步长的1-D tensor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0.]])"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.zeros(3,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.,  1.,  1.],\n",
       "        [ 1.,  1.,  1.],\n",
       "        [ 1.,  1.,  1.]])"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.ones(3,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.,  0.,  0.],\n",
       "        [ 0.,  1.,  0.],\n",
       "        [ 0.,  0.,  1.]])"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.eye(3,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.,  2.,  4.,  6.,  8.])"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.arange(start=0, end=10, step=2)#区间是左壁右开，不包含end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `torch/Tensor.round(input, out=None)` → Tensor\n",
    "对tensor内所有元素取整"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 7.,  5.],\n",
       "        [ 1.,  5.]])"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.randint(0,10,(2,2))\n",
    "a.round()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `torch.random`torch自带随机模块\n",
    "生成随机数的功能模块\n",
    "### `torch.random.manual_seed(seed)`\n",
    "设置随机数种子\n",
    "### `torch.randint(low, high, size)`\n",
    "返回一个服从uniform分布的int随机矩阵\n",
    "### `torch.randn(*sizes, out=None, dtype=None, layout=torch.strided, device=None, requires_grad=False)`\n",
    "$\\text{out}_{i} \\sim \\mathcal{N}(0, 1)$\n",
    "生成一个服从(0,1)标准正态分布的矩阵,传入size使用参数收集机制。\n",
    "### `torch.randperm(n, out=None, dtype=torch.int64, layout=torch.strided, device=None, requires_grad=False) → LongTensor`\n",
    "生成一个`0→n-1`随机序列\n",
    "**参数：**\t\n",
    "\n",
    "    n (int) – 生成序列最大上界(不包含n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 6.,  9.],\n",
       "        [ 3.,  8.]])"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.randint(0, 10, (2,2))#使用参数收集机制"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.7565,  1.2032],\n",
       "        [-0.7030,  1.7259]])"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.randn(2,2)#使用参数收集机制，服从搞死分布"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0,  1,  2,  3])"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.randperm(4)#返回一个生成的序列"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9], dtype=torch.int32)"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.arange(0,10,1).type(torch.int32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# `torch.Tensor`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `backward(gradient=None,  retain_graph=None, create_graph=False)`\n",
    "计算当前tensor的梯度\n",
    "根据链式传播法则传播梯度，如果当前tensor\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `detach()`\n",
    "返回一个新的Tensor，脱离原来计算梯度图\n",
    "结果讲不会再需要梯度\n",
    ">Note:返回的Tensor与原来共享相同的数据"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `grad`\n",
    "默认是None，存储梯度信息。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
